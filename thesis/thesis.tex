\documentclass[draft,final]{thesisclass} % Remove option 'final' to obtain debug information.

% Load packages to allow input and output of non-ASCII characters.
\usepackage{lmodern}        % Use an extension of the original Computer Modern font to minimize the use of bitmapped letters.
\usepackage[T1]{fontenc}    % Determines font encoding of the output. Font packages have to be included before this line.
\usepackage[utf8]{inputenc} % Determines encoding of the input. All input files have to use UTF8 encoding.

% added package for bibliography
\usepackage{csquotes}
\usepackage[backend=biber,style=authoryear,date=year,maxbibnames=10,dashed=false]{biblatex}
\addbibresource{thesis.bib}
\DeclareDelimFormat[bib,biblist]{nametitledelim}{\addcolon\space}
\DeclareDelimFormat{postnotedelim}{\addcolon\space}

% for definitions
\usepackage{amsthm}
\newtheorem{definition}{Definition}

% Extended LaTeX functionality is enabled by including packages with \usepackage{...}.
\usepackage{amsmath}    % Extended typesetting of mathematical expression.
\usepackage{amssymb}    % Provides a multitude of mathematical symbols.
\usepackage{mathtools}  % Further extensions of mathematical typesetting.
\usepackage{microtype}  % Small-scale typographic enhancements.
\usepackage[inline]{enumitem} % User control over the layout of lists (itemize, enumerate, description).
\usepackage{multirow}   % Allows table elements to span several rows.
\usepackage{booktabs}   % Improves the typesetting of tables.
\usepackage{subcaption} % Allows the use of subfigures and enables their referencing.
\usepackage[ruled,linesnumbered,algochapter]{algorithm2e} % Enables the writing of pseudo code.
\usepackage[usenames,dvipsnames,table]{xcolor} % Allows the definition and use of colors. This package has to be included before tikz.
\usepackage{nag}       % Issues warnings when best practices in writing LaTeX documents are violated.
\usepackage{todonotes} % Provides tooltip-like todo notes.
\usepackage{hyperref}  % Enables hyperlinking in the electronic document version. This package has to be included second to last.
\usepackage[acronym,toc]{glossaries} % Enables the generation of glossaries and lists of acronyms. This package has to be included last.
\usepackage{lipsum}  % Provides blind text.
\usepackage{acronym} % Provides a list of acronyms.
\usepackage{float} % Provides the H float modifier option.
\usepackage{tabularx} % Provides a tabular package.
\usepackage{listings} % for code listings.

% Define convenience functions for using the author name and the thesis title in the PDF document properties.
\newcommand{\authorname}{Hannes Brantner} % The author name without titles.
\newcommand{\thesistitle}{Enhancing recruitment efficiency by exploring the impact of large language models on the screening process} % The thesis title. The English version should be used if it exists.

% Set PDF document properties
\hypersetup{
    pdfpagelayout   = TwoPageRight,           % How the document is shown in PDF viewers (optional).
    linkbordercolor = {Melon},                % The color of the borders of boxes around hyperlinks (optional).
    pdfauthor       = {\authorname},          % The author's name in the document properties (optional).
    pdftitle        = {\thesistitle},         % The document's title in the document properties (optional).
    pdfsubject      = {LLMs in \acs{HR}},              % The document's subject in the document properties (optional).
    pdfkeywords     = {Machine Learning, \acs{HR}, Human Resources, \acs{AI}, LLM} % The document's keywords in the document properties (optional).
}

\setpnumwidth{2.5em}        % Avoid overfull hboxes in the table of contents (see memoir manual).
\setsecnumdepth{subsection} % Enumerate subsections.

\nonzeroparskip             % Create space between paragraphs (optional).
\setlength{\parindent}{0pt} % Remove paragraph indentation (optional).

\makeindex      % Use an optional index.
\makeglossaries % Use an optional glossary.
%\glstocfalse   % Remove the glossaries from the table of contents.

% Set persons with four arguments:
%  {title before name}{name}{title after name}{gender}
%  where both titles are optional (i.e., can be given as empty brackets {}).
\setauthor{Ing. Dipl.-Ing.}{\authorname}{}{male}
\setadvisor{Mag. Dr.}{Alexander Pfeiffer}{MBA MA}{male}

% For bachelor and master theses:
\setfirstassistant{}{Michaela Wawra}{MSc}{female}
% \setsecondassistant{Pretitle}{Forename Surname}{Posttitle}{male}
% \setthirdassistant{Pretitle}{Forename Surname}{Posttitle}{male}

% For dissertations:
% \setfirstreviewer{Pretitle}{Forename Surname}{Posttitle}{male}
% \setsecondreviewer{Pretitle}{Forename Surname}{Posttitle}{male}

% For dissertations at the Ph.D. School and optionally for dissertations:
% \setsecondadvisor{Pretitle}{Forename Surname}{Posttitle}{male} % Comment to remove.

% Required data.
\setregnumber{01614466}
\setauthorbirthdate{19.03.1998}
\setauthorbirthplace{Mistelbach}
\setdate{01}{04}{2024} % Set date with 3 arguments: {day}{month}{year}.
\settitle{\thesistitle}{\thesistitle} % Sets English and German versions of the title (both can be English or German). If your title contains commas, enclose it with additional curvy brackets (i.e., {{your title}}) or define it as a macro as done with \thesistitle.
\setsubtitle{}{} % Sets English and German versions of the subtitle (both can be English or German).

% Select the thesis type: bachelor / master / doctor.
% Bachelor:
% \setthesis{bachelor}
%
% Master:
\setthesis{master}
\setmasterdegree{master} % dipl. / rer.nat. / rer.soc.oec. / master
%
% Doctor:
%\setthesis{doctor}
%\setdoctordegree{rer.soc.oec.}% rer.nat. / techn. / rer.soc.oec.

% For bachelor and master:
\setcurriculum{Master in Business Administration}{Master in Business Administration} % Sets the English and German name of the curriculum.

% Optional reviewer data:
\setfirstreviewerdata{Affiliation, Country}
\setsecondreviewerdata{Affiliation, Country}

% Add glossary entries.
\newglossaryentry{LLM}
{
    name=Large Language Model,
    description={A Large Language Model is reading and emitting text, enabling it to perform tasks such as translation, summarization, and question answering}
}
\newglossaryentry{TTH}
{
    name=time-to-hire,
    description={The time from the receiving of the candidate's application to the accepted job offer}
}
\newglossaryentry{TAPJFNN}{
    name=Topic-Based Ability-Aware Person-Job Fit Neural Network,
    description={This framework based on the Recurrent Neural Network architecture for predicting person-job fit was introduced in \textcite{pj_fit_ml}}
}

% define style for Javascript scripts
\lstdefinelanguage{TypeScript}{
  keywords={typeof, new, true, false, catch, function, return, null, catch, switch, var, if, in, while, do, else, case, break},
  keywordstyle=\color{blue}\bfseries,
  ndkeywords={class, export, boolean, throw, implements, import, this},
  ndkeywordstyle=\color{darkgray}\bfseries,
  identifierstyle=\color{black},
  sensitive=false,
  comment=[l]{//},
  morecomment=[s]{/*}{*/},
  commentstyle=\color{purple}\ttfamily,
  stringstyle=\color{red}\ttfamily,
  morestring=[b]',
  morestring=[b]"
}
\lstset{
  basicstyle=\ttfamily,
  columns=fullflexible,
  frame=single,
  breaklines=true,
  postbreak=\mbox{\textcolor{red}{$\hookrightarrow$}\space},
  captionpos=b,
}

\begin{document}

\frontmatter % Switches to Roman numbering.
% The structure of the thesis has to conform to the guidelines at
%  https://informatics.tuwien.ac.at/study-services

% \addtitlepage{naustrian} % German title page.
\addstatementpage
\addtitlepage{english} % English title page.

% citation tutorial
% cite only page 1
% \parencite[1]{discrimination_algorithms} \newline
% cite pages 2 to 5
% \parencite[2-5]{discrimination_algorithms} \newline
% cite page 3 and the following page
% \parencite[3f]{discrimination_algorithms} \newline
% cite page 3 and the following pages
% \parencite[3ff]{discrimination_algorithms} \newline

\begin{acknowledgements}
\lipsum[1]
\end{acknowledgements}

% Use an optional list of figures.
\listoffigures % Starred version, i.e., \listoffigures*, removes the toc entry.
\cleardoublepage

% Use an optional list of tables.
\listoftables % Starred version, i.e., \listoftables*, removes the toc entry.
\cleardoublepage

\chapter{List of Abbreviations}
% Add acronym entries.
\begin{acronym}
    \acro{AI}{Artificial Intelligence}
    \acro{CV}{Curriculum Vitae}
    \acro{LLM}{Large Language Model}
    \acro{LIWC}{Linguistic Inquiry and Word Count}
    \acro{HR}{Human Resources}
    \acro{NLP}{Natural Language Processing}
    \acro{TAPJFNN}{Topic-Based Ability-Aware Person-Job Fit Neural Network}
\end{acronym}
\cleardoublepage

% Select the language of the thesis, e.g., english or naustrian.
\selectlanguage{english}

% Add a table of contents (toc).
\tableofcontents % Starred version, i.e., \tableofcontents*, removes the self-entry.

\chapter{Executive Summary}
\lipsum[1]
\cleardoublepage

\begin{abstract}
\lipsum[1]
\end{abstract}

% Switch to Arabic numbering and start the enumeration of chapters in the table of contents.
\mainmatter

\chapter{Introduction} \label{introduction}

\section{Problem Statement} \label{problem_statement}
This section relies heavily on the article \textcite{ai_recruiting}, which is based on over $50$ research papers in that field and concisely describes the evolution of recruiting.
As described in \textcite[1]{ai_recruiting}, the average firm's value by 2000 comprised roughly 65\% of value from intangible assets.
This has evolved a lot, as by the end of 1980, around 70 to 90\% of tangible assets were accountable for the average firm's value \parencite[1]{ai_recruiting}.
This means that a firm's value is increasingly dependent on the quality of its employees and the knowledge they possess, and this ongoing process does not seem to halt soon.
The technological context of how companies recruit people has also evolved a lot over the last decades, as machine learning tools are more and more used to automate processes or assist humans in attracting suitable candidates, screening, assessing, and selecting them \parencite[2]{ai_recruiting}.
The article \textcite[2-4]{ai_recruiting} described four evolution stages of recruiting, which are the following:
\begin{enumerate}
    \item \textbf{Analog Recruiting} \label{analog_recruting}\\
    The first stage is analog recruiting, where people are the primary mechanism of recruiting new employees.
    Prospective applicants must go to the company to manually submit a paper job application.
    Companies want to maximize the information richness they supply for the current job vacancy, but also, the information reaches to attract as many promising applicants as possible.
    But as described in \textcite[2]{ai_recruiting}, in analog recruiting, these two goals faced the analog reach and richness frontier as shown in \ref{fig:analog_reach_richness_frontier} because the more information is supplied, the more costly it is when you supply to media with great reach at that time:
    \begin{figure}[H]
        \centering
        \includegraphics[scale=0.5,page=2,width=0.6\linewidth,trim={300 100 55 515},clip]{literature/ai_recruiting.pdf}
        \caption{Analog reach and richness frontier \parencite[2]{ai_recruiting}}
        \label{fig:analog_reach_richness_frontier}
    \end{figure}
    \item \textbf{Digital Recruiting 1.0} \label{digital_recruiting_1}\\
    This first evolution stage of digital recruiting in the late 1990s was characterized by breaking the analog reach and richness frontier by using digital media, which allowed the supply of rich information to many prospective candidates at a meager cost because there were no printing and low distribution costs.
    There was also a change on the applicant's side as no more manually filled-out documents were required to be handed in physically at the company site.
    It also allowed them to filter out job offers based on various selectors, and it also allowed companies to offer more dynamic content to job seekers by adding video and audio data to the job search websites.
    A considerable exponential and self-enforcing network effect was also ongoing for job search websites \parencite[3]{ai_recruiting}.
    As they listed more job offers, more job seekers were attracted to the platform, which made it easier to persuade companies to list their job offers on job search websites.
    \item \textbf{Digital Recruiting 2.0} \label{digital_recruiting_2}\\
    This evolution emerged around ten years after the first evolution of digital recruiting and was mainly driven by two developments.
    The first development was the rise of job board aggregation services that presented all job offers from various job search websites on one platform.
    Job seekers can now access all available job offers without visiting each platform individually. Job firms do not need to publish their offers on each website.
    The second development was the introduction and rise of professional social network platforms such as \textit{LinkedIn}.
    These network platforms allowed people to form professional communities and groups of interest and also allowed companies to present themselves to the public and potential applicants.
    Moreover, with endorsements, people on such platforms could endorse the skills of others, which allowed them to build a reputation and trust in the community.
    It enabled companies to target their job ads more effectively to promising candidates, contact candidates directly through the network, and also helped a cheap and efficient way to post job offers on large social websites, whether they are professional or not, like \textit{Facebook} \parencite[3]{ai_recruiting}.
    \item \textbf{Digital Recruiting 3.0} \label{digital_recruiting_3}\\
    After Digital Recruiting matured from 2010 to 2015, Digital Recruiting 3.0 emerged and was primarily driven by the rise of machine learning and artificial intelligence in human resources processes.
    As digital recruiting digitized the processes and made them more frictionless for employers and employees, more and more applications came in for each job offer. On average, the number is around 250 applications per job offer \parencite[4]{ai_recruiting}. 
    This can be explained by the fact that the application cost is meager for the applicant, but this led to about 80\% of the applications being unqualified \parencite[4]{ai_recruiting}.
    To deal with this massive number of applicants, you can either deploy more human resources to screen the applications or use machine learning to automate processes, or assist humans so that they can work more productively.
    Human resources are mission-critical for companies as human capital is increasingly critical for success.
    Moreover, research showed that top performers are four to eight times more productive than the average performers, increasingly so in complex environments, according to \textcite[4]{ai_recruiting}.
    Furthermore, this new evolution stage of recruiting also has its downsides, as discussed in \textcite[4-5]{challenges_opportunities_hr} due to the emerging use of technology:
    \begin{itemize}
        \item It was stated that using technology in human resources typically leads to more efficiency and decreasing costs associated with human resources transactions. However, some researchers argue that there is no considerable effect on the primary goal of human resources to attract, motivate, and maintain talented employees. 
        \item Furthermore, it is argued that the technologies are often static and one-way communication systems that do not allow either side of the recruiting process to ask questions. This may entail creating an artificial distance between the applicant and the people doing the recruitment tasks. The technologies of the future, which are already here, can partially mitigate those drawbacks.
        \item The article also discussed that if the tasks of complete human resource departments are transferred to other employees and managers, this may hurt the overall productivity of organizations. This also means that if the administrative burden for the human resource department is reduced, it can also contribute to the strategic direction of organizations.
    \end{itemize}
    More consideration on the use of \acs{AI}-assisted technologies regarding recruiting are presented here \parencite[2-4]{ai_in_hr_management}:
    \begin{itemize}
        \item Algorithms that help filter applicants and were trained with imitation learning are often trained with biased data. As these algorithms try to map input attributes or features to desirable outputs like job performance present in the training data sets, they keep the bias from past recruiting activities, such as hiring fewer women in top management positions in favor of men. Also, \textit{Amazon} had this bias problem in their hiring algorithm, which is an alarming sign for using \acs{AI} in human resources due to legal considerations and violations of social norms.
        \item Furthermore, as applicants discover the biases of the used algorithms or traits they will like, they may increasingly artificially craft application documents to meet the algorithm's expectations.
        \item The article also discussed that it is hard to define what a \textit{good employee} is. Most sources describe an excellent employee based on performance appraisal scores. Still, those are very hard to measure as any reasonably complex job is interdependent with other jobs, making it very hard to disentangle individual performance from group performance. The performance metrics are often criticized for a lack of validity and reliability while having a considerable bias. Therefore, it is challenging and nearly impossible to achieve precisely this behavior of selecting high-performant applicants with imitation learning and unsuitable training data.
        \item Algorithms based on imitation learning require large amounts of past data, and important events like dismissals may be sparse in the training data.
        \item Moreover, algorithms are not liable for their decisions, even though hiring activities greatly affect the organization’s dynamics. Most algorithms in use today cannot explain what attributes have driven the final decision.
    \end{itemize}
\end{enumerate}
This should have made the need for an effective human resources pipeline clear. To improve this pipeline, machine learning tools are deployed to the following four fields \parencite[4-8]{ai_recruiting}:
\begin{enumerate}
    \item \textbf{Outreach}\\
    Firms try to identify candidates and get job opportunities in front of them in a way that invites them to apply. Machine learning can help refine the job description in ways that attract more applicants or bring more balance to the gender of the applicants. As there are more passive candidates not actively searching for a job than actively searching candidates, machine learning can identify suitable talents from a massive pool of candidates, e.g., hundreds of millions of \textit{LinkedIn} users.
    \item \textbf{Screening}\\
    This stage should pre-filter the applicants to keep only the most promising ones for the following assessment stage.
    As stated in \textcite[6]{ai_recruiting}, machine learning tools were at least 25\% superior to humans even when they had a reasonable amount of time to screen the application. Furthermore, the \acs{AI}-enabled screening tool provider \textit{Ideal} says that with its technology, the average \gls{TTH} fell from 24 days to 9 days.
    Moreover, \textit{L'Oréal} reported a drop of screening time per resume from 40 minutes to 4 minutes, and \textit{Hilton Hotels \& Resorts} reported a decline of \gls{TTH} from 42 days to 5 days by incorporating \acs{AI} tools in the recruitment process.
    \item \textbf{Assessment}\\
    The assessments typically involve one to many rounds of evaluation to determine the most suitable candidates who will receive a job offer. Recent research showed that gamification with short games is increasingly used to measure candidates' personality traits, like risk aversion. Companies like \textit{Unilever} and \textit{L'Oréal} used \acs{AI}-chatbots that asked various questions, and applicants were able to record video responses in the \textit{Unilever} case and chat response in the case of \textit{L'Oréal}. The \acs{AI} systems analyzed the content, the word choice, and the used structure and matched the responses against successful employees in that field. In the case of video content, the system can also analyze the tone of the voice and the micro-facial movements. These two chatbots could be filled with information at any time within the given timeframe, reducing scheduling times with candidates and offering candidates more freedom to answer the questions at a time that suits them best. These chatbots can also be used to fill in missing information from job seekers, like potential start dates, and answer questions regarding the salary range.
    \item \textbf{Coordination}\\
    This task involves coordinating with the applicants, which requires appointment scheduling and cancellation. In the age of digital recruitment, more and more candidates are rejected. Therefore, you need to convey this information to all of your applicants, as little to no information regarding the process is the main driver for bad experiences with the application process. Machine learning tools can help to achieve that.
\end{enumerate}
By 2018, only around 40\% had used machine learning tools in these four core human resource sourcing processes \parencite[4]{ai_recruiting}.
The cost to implement, integrate, and maintain \acs{AI} tools in human resources is significant, and most companies should use services from external providers if they do not have massive amounts of hires to amortize these costs \parencite[8]{ai_recruiting}.
Furthermore, suppose the \acs{AI} tools are not used as assistive technology but as a technology to replace humans in the human resource sector. In that case, the acceptance of these technologies within this sector is relatively low \parencite[9]{ai_recruiting}.
Moreover, around 70\% of large organization change initiatives, including digital transformations, fail.
That means there is some space for improvement, and this thesis tries to implement a new open-source tool in screening to improve the productivity of human resources workers in companies of any size.
As \acs{LLM}s are capable of summarization and question-answering when they are presented with natural text as input within their context length, the problem to solve now is to match a provided \acs{CV} from an applicant to the applied job description and assign this match a score.
Furthermore, the model should also characterize the applicant as promising or not using an additional output.
The score should represent the candidate's suitability for the job description and is needed to compare candidates easily and sort them by their score.

\section{Significance of the Work} \label{significance_of_the_work}
The advantages of Digital Recruiting 3.0 should be available to all companies and not just to big corporations or as paid services from external providers.
Machine learning tools should also not have the precondition of having massive datasets to train and use the models.
Using the model should bring efficiency benefits without using it often to amortize costs, making it perfectly viable for small companies.
The effects and the significance of using \acs{AI}-enabled technologies in human resource management have been discussed in detail in \textcite{ai_hrm_review} by reviewing $45$ articles in that field and covering the following main points:
\begin{itemize}
    \item The article describes a disruptive (future) transformation from electronic human resource management systems to systems defined by intelligent automation. The ongoing digitization of information was an enabler for this transformation \cite[4]{ai_hrm_review}.
    \item It is also discussed that automatic matching of candidates to jobs reduces costs and the need for \acs{HR} professionals to have domain knowledge for a particular professional field \cite[10]{ai_hrm_review}.
    \item Furthermore, as automated tools eliminate distance constraints, there is an increasing risk of lacking direct contact between people, which may hide lurking problems. Therefore, they propose to use the \acs{AI} tools as assistive technologies \cite[12]{ai_hrm_review}.
    \item There is also a theory presented that \acs{AI} will not replace human workplaces on the job level from the start but rather on the task level, so more straightforward tasks will be automated first, and more complex tasks will be automated later \cite[12]{ai_hrm_review}. For example, simple question-answering or administrative work can already be executed by voice assistants like \textit{Siri} without having human employees as representatives at physical locations.
    \item \acs{AI}-based application could significantly improve employee training or applicant assessing because they can be used to simulate real-world scenarios with supported interactivity within a safe environment \cite[13]{ai_hrm_review}. In the future, the automated screening procedure in the proposed thesis model can also be amended with feedback from the applicant to further clarify skills that are a missing education requirement, for example.
    \item The article \parencite[14]{ai_hrm_review} also highlights the importance of explanations from \acs{AI} systems on decisions they have made for a human to understand the automated decision. This feature was also integrated into the model proposed by this thesis.
    \item The proposed model of this thesis is based on \acs{LLM} models with fixed training sets that only contain information up to a certain point in time. Still, the article \textcite[13]{ai_hrm_review} emphasizes the importance of an up-to-date model that was trained or had access to real-time data, even if this means continuous readjustments to the model.
\end{itemize}


\section{Research Objective and Research Question} \label{research_objective_and_research_question}
The research aims to supply the proposed reasonably accurate screening tool based on \acs{LLM}s outlined in \ref{problem_statement} as open-source software to the public. 
That means companies of any size can integrate this tool into their human capital sourcing pipelines to increase efficiency and decrease costs eventually.
As outlined in the previous chapters, people working in human resources have to screen more and more resumes or \acs{CV}s as the reach of online job advertisements attracts many applicants.
It was also discussed that many of these applicants were not qualified for the advertised job and should not be assessed further.
This pre-filtering saves costly human resources and makes it easier to invest the most time and effort into the most promising candidates, given that you have an easy and cheap way to screen the application documents.
With the rise of \acs{LLM}s capable of summarization and question answering, the idea was born to use these capabilities and apply them to the screening task in human resources.
\\\\
The research question is:\\
\textit{How might an \acs{LLM}-based screening model affect the screening process?}\\\\
This formulation entails subordinate research questions, which are shown in the following enumeration:
\begin{itemize}
    \item \textit{What is the accuracy of the proposed model?}\\
    This question will be answered by testing the model's outputs' accuracy against human expert recruiters' annotations.
    This procedure is described in detail in \ref{quantitative_research_design}.
    \item \textit{What time savings are associated with the model usage?}\\
    This question will be answered by measuring the time the human experts took to execute the data mentioned above annotation.
    This time will be compared to a sampled time it takes for users to automate the same tasks using the model.
\end{itemize}

\section{Research Methodology} \label{research_methodology}
This thesis uses qualitative research design \ref{qualitative_research_design} and quantitative research design \ref{quantitative_research_design} to answer the research question.
In the qualitative context, the current state of experiences with and expectations from \acs{AI}-assisted tooling will be explored by a discussion with human experts in the human resources field.
In the quantitative context, the accuracy of the proposed model will be measured, and the time savings associated with the model usage will be calculated by comparing the model's statistics against the results from human expert recruiters.

\section{Structure of the Work} \label{structure_of_the_work}
This thesis starts with the introduction chapter \ref{introduction} which gives an overview of the problem statement \ref{problem_statement}, the significance of the work \ref{significance_of_the_work}, the research objective and research question \ref{research_objective_and_research_question}, the research methodology \ref{research_methodology} and the structure of the work \ref{structure_of_the_work}.
The second chapter \ref{theoretical_background} gives an overview of the theoretical background of the work, like quickly revisiting \gls{LLM}s and then discussing literature on person-environment fit and methods to quantize this fit.
Afterward, the methodology chapter \ref{methodology} describes what research methods have been used, how the results have been obtained, how the sample screening documents were created, and how the model has been created.


\chapter{Theoretical Background} \label{theoretical_background}

\section{Large Language Models}
The machine learning model type \gls{LLM} is commonly abbreviated as \acs{LLM} and makes natural language texts processable for computers.
The model works by compressing the read text into an internal state, and based on this state, an output text is generated.
As the model compresses the data according to the learned patterns in the training data, it does not understand the text the same way humans do.
These models perform tasks such as translation, summarization, and question answering \parencite[1]{llm_literature_review}.
As this enabled a wide field of automation for applications that humans primarily carried out, many technology companies have developed their own \acs{LLM} and made their services available to the public while keeping their source code private.
Some notable examples are the \textit{Gemini} model family by \textit{Google} \parencite{gemini}, the \textit{GPT-4} model by \textit{OpenAI} \parencite{gpt4} and the \textit{LLAMA 2} model family by \textit{Meta} \parencite{llama2}.
The models of the \textit{LLAMA 2} model family are the only models from the picked three candidate groups that have been released with public source code.
That means that researchers and engineers worldwide can use the model as a building block, improve the underlying mechanisms, or try to incorporate it into their applications.
The real breakthrough of \acs{LLM}s came with the release of the \textit{GPT-3} model by \textit{OpenAI} \parencite{gpt3} that powered the initial version of \textit{ChatGPT}.
They offered the model's capabilities as a website accessible to the public and allowed users to insert text and receive a response from the model.
As described in \textcite[1]{gpt3}, the model provided excellent performance on various \acs{NLP} datasets that include tasks like translation and question answering.
To briefly visualize the complexity behind a \acs{LLM}, consider the function $f_{a,b,c}(x) = ax^2+bx+c$ that has the three parameters $a$, $b$ and $c$.
Here, the function $f$ maps the input data denoted as $x$ to the output data denoted as $f(x)$.
The input data in the \acs{NLP} case is the input text, and the output data is the output text.
\acs{LLM}s also use parameterized and differentiable functions in their internal structure to map the input text to the output text. Still, they have orders of magnitudes more parameters than this sample function.
For example, the model \textit{GPT-3} uses 175 billion parameters \parencite[1]{gpt3} in total. The parameter count of \textit{GPT-4} was not disclosed in their technical report \parencite{gpt4}.
The training data needed to train these models was present in the required amounts because the researchers have yet to use reinforcement learning \parencite{rl_bible} or supervised learning \parencite[3]{sl_bible}.
Reinforcement learning gives the model a reward when an action is good and a punishment when an action is wrong. This can be done by letting the model train in the real world, which has terrible scalability, or in simulators that must be coded to mimic the real world, which is also challenging and time-consuming.
For example, animals learn that behaviors that lead to food are good and behaviors that lead to hunger are bad.
The supervised learning approach labels input data with the desired output label, and based on these mappings and a vast number of training samples, the model will adjust its parameters to map the input data correctly.
An example would be input images labeled with the object that can be seen in the pictures. So, each input image must be labeled by hand before the model can be trained to predict itself correctly.
To compress the patterns in the training data and fill them into the model parameters, vast amounts of training data are needed as the parameter count of \acs{LLM}s is enormous.
Both techniques were insufficient to train models of that sheer size, so the researchers used a method called self-supervised learning \parencite[7]{llm_literature_review} where the model tries to fill in artificially generated gaps in the text.
Because the gaps are artificially introduced, the expected output is known in advance, so the model can be trained without the need for labeled training data, which makes this technique very scalable.
Before the text is fed to the model, it is tokenized \parencite[4]{llm_literature_review}, which means the text is converted into a sequence of tokens, which can either be symbols, characters, subwords, and words.
The model gets the tokenized text input and predicts the follow-up output tokens that will likely follow these input tokens. The output tokens are converted back into text, and then the text from the artificial gap is compared to the model output.
Based on this comparison, the model parameters are adjusted. As no human data labeling or costly real-world or simulator training is needed, the model can be trained on vast amounts of data.
Most text data for training large language models is scraped from the internet \parencite[1]{llm_literature_review}.
The context size of a \acs{LLM} is the number of tokens the model can store in its internal state while generating the output tokens, so it is the history of tokens it can access while generating the response.
For example, the context size of the \textit{LLAMA 2} model is 4096 tokens \parencite[47]{llama2}.
The tokenizer that \textit{OpenAI} is using can be accessed via this website \textcite{openai_tokenizer}.
The output tokens of a \acs{LLM} are generated token by token, so when the first token is generated, it is appended to the input, and the model generates the next token based on the input that now contains the first generated token. Afterward, it is run iteratively in the same way.
That means that the total output token size is also constrained by the context size when the entire history of tokens should be within the context size while generating the output.
The model stops the output loop when the model emits the stop token. That means that it has finished the output token generation.
Most \acs{LLM}s are based on the machine learning model type called Transformers \parencite[1]{transformer}, which is a neural network architecture that is based on the attention mechanism.

\section{Person-Environment Fit}
The person-environment fit can be broken up into the person-organization and the person-job fit.
For this section, many citations are made from the article \textcite{po_and_pj_fit_literature_review} as it is an excellent summary of more than $90$ research papers in this field.
Person-organization fit describes the compatibility between the applicant's and the organization's values and characteristics and how well they meet each other's needs \parencite[179]{po_and_pj_fit_literature_review}.
The person-job fit describes the compatibility between the applicant's abilities and the job's demands or the person's desires and the job's attributes \parencite[179]{po_and_pj_fit_literature_review}.
These two fit measures should both be considered when computing the score.

\subsection{Person-Environment Fit Types} \label{pef_types}
In essence, person-environment fit is a complex and multi-dimensional concept.
It can be conceptualized as both complementary and supplementary. 
Supplementary fit happens when a person supplements, embellishes or possesses characteristics similar to others in the environment \parencite[180]{po_and_pj_fit_literature_review}.
Complementary fit happens when a person possesses missing characteristics in the environment, making it more complete \parencite[180]{po_and_pj_fit_literature_review}.
Complementary person-environment fit encompasses the needs-supplies and demands-abilities perspectives. 
An environment supplies financial, physical, and psychological resources and task-related, interpersonal, and growth opportunities that the person demands or needs \parencite[180]{po_and_pj_fit_literature_review}.
A needs-supplies fit is achieved when the supply meets the individual's needs.
A demands-abilities fit is achieved when the individual's professional contributions in terms of time, effort, commitment, skills, and knowledge meet the demands of the environment \parencite[180]{po_and_pj_fit_literature_review}.
Person-environment fit can be understood as perceived fit and actual fit.
The interconnections between these types of person-environment fit are visualized in figure \ref{fig:person_environment_fit_types}:
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.5,page=3,width=0.8\linewidth,trim={55 130 55 470},clip]{literature/po_and_pj_fit_literature_review.pdf}
    \caption{Person-environment fit types \parencite[3]{po_and_pj_fit_literature_review}}
    \label{fig:person_environment_fit_types}
\end{figure}

\subsection{Person-Organization Fit}
Person organization-fit is defined as the compatibility between people and organizations or between an individual and broader organizational attributes and is the key to maintaining a flexible and committed workforce in competitive business environments with probably tight labor markets \parencite[182]{po_and_pj_fit_literature_review}.
The roots of person-organization fit research can be traced to Schneider's Attraction-Selection-Attrition framework, which says that organizations are one particular situation where people are attracted to specific job opportunities, selected to be a part of and remain within the organization as long as they are a good fit \parencite[182]{po_and_pj_fit_literature_review}.
According to \textcite[182]{po_and_pj_fit_literature_review}, person-organization fit can be operationalized in four ways. The following list is ordered from most used to least used operationalization:
\begin{enumerate}
    \item \textbf{Congruence between individual and organizational values}\\
    This operationalization primarily encompasses the supplementary fit perspective that was introduced in section \ref{pef_types}.
    \item \textbf{Goal congruence with organizational leaders and peers}\\
    This operationalization primarily encompasses the supplementary fit perspective that was introduced in section \ref{pef_types}.
    \item \textbf{Match between individual preferences or needs and organizational systems and structures}\\
    This operationalization reflects the needs-supply fit perspective that was introduced in section \ref{pef_types}.
    \item \textbf{Match between the characteristics of individual personality and organizational climate}\\
    Organizational climate is also sometimes labeled as organizational personality.
    As the organizational climate is often operationalized in terms of corporate supplies (like communication patterns and reward systems), this operationalization also reflects the needs-supply and the supplementary fit perspectives introduced in section \ref{pef_types}.
\end{enumerate}
Empirical evidence has shown that a high level of person-organization fit is associated with higher job satisfaction, organizational commitment, self-reported teamwork, and objective measures for work performance \parencite[183]{po_and_pj_fit_literature_review}.
A high person-organization fit is associated with lower turnover intentions and actual turnover. However, some researchers have also pointed out that a high level of person-organization fit may have negative organizational outcomes \parencite[183]{po_and_pj_fit_literature_review}.

\subsection{Person-Job Fit}
The concept of person-job fit is the traditional foundation for employee selection, and the primary concern has been finding the applicants with skills and abilities necessary to do the job \parencite[183]{po_and_pj_fit_literature_review}.
This encompasses the demands-abilities fit perspective that was introduced in section \ref{pef_types}.
Person-job fit can be assessed by determining the job demand through a job analysis, which identifies an employer's essential tasks and uses this knowledge to specify the skills, knowledge, and abilities to complete the job tasks.
Determining person-job fit increasingly gained sophistication by identifying statistically reliable and valid methods to assess the fit between the person and the job \parencite[183]{po_and_pj_fit_literature_review}.
This assessment also received legal support and was merged into the employee selection procedure, and this fit measure can be operationalized in the following way \parencite[183-184]{po_and_pj_fit_literature_review}:
\begin{itemize}
    \item \textbf{Needs-supplies and demands-abilities perspective}\\
    These two perspectives cover the complementary fit concept that was introduced in section \ref{pef_types}.
    The supplementary fit concept does not suit the person-job fit context as this model compares the applicant to other people rather than against the job.
    The needs-supplies perspective includes the individual's desires like goals, psychological needs, interests, and values. The characteristics and attributes of a job may or may not satisfy those desires.
    The demands-abilities perspective consists of job demands like knowledge, skills, abilities, education, experience, and aptitudes required to carry out the tasks of this job.
\end{itemize}
The strategies to assess the person-job fit include resumes, \acs{CV}s, tests, interviews, and reference checks \parencite[184]{po_and_pj_fit_literature_review}.
The more structured and validated the procedures were, the more effective the employee selection process compared to using unstructured strategies like oral interviews with no guidelines \parencite[184]{po_and_pj_fit_literature_review}.
Moreover, most candidate selection processes of companies have focused on achieving a high person-job fit because this has positive outcomes like job satisfaction, lower stress levels, and higher adjustment, organizational commitment, motivation, performance, attendance, and employee retention \parencite[184]{po_and_pj_fit_literature_review}.

\subsection{Relationship between Person-Organization Fit and Person-Job Fit}
Research showed that the discriminant validity of these two types is given, as empirical studies have shown that the correlation between person-organization and person-job fit, whether perceived or actual, is very low \parencite[185]{po_and_pj_fit_literature_review}.
Using confirmatory factor analysis, it was shown that job recruiters and applicants could distinguish between the two types of fit, but it was also demonstrated that the recruiters' perceived fit, no matter what kind, is influenced by their past experiences in a predictable way \parencite[185]{po_and_pj_fit_literature_review}.
It was also shown that person-organization fit has a more significant favorable influence on employee retention and contextual performance than person-job fit \parencite[185]{po_and_pj_fit_literature_review}.
There can even be a third type called person-group fit, which is like person-organization fit but limited to a specific group within the organization the employee is working with.
Furthermore, these three types of fit uniquely impact various metrics, supporting the previously made claim that they are distinct concepts \parencite[185]{po_and_pj_fit_literature_review}.

\subsection{Person-Environment Fit in Employee Selection}
Research on employee selection can be divided into the following two fields \parencite[185-186]{po_and_pj_fit_literature_review}:
\begin{itemize}
    \item \textbf{Prescriptive approach}\\
    The prescriptive approach focuses on guidelines that describe selecting the right candidate for the job.
    Traditionally, the selection process has been based mainly on the concept of the person-job fit.
    However, the paradigm shifted, and in addition to the task performance of the candidate, the contextual performance of the candidate is also considered, which is mainly influenced by the person-organization fit of the candidate.
    Researchers have argued that selecting employees with a high person-organization fit may be beneficial, as this means unity in values and visions.
    Static job analysis must be revised in many industries as jobs are dynamic and ever-evolving. Therefore, it is better to recruit a flexible workforce capable of teamwork.
    The person-job fit should also focus more on the applicant's general cognitive ability, which is justified by the fact that most employers will hold multiple jobs within a company. Therefore, a more significant focus on general cognitive ability will lead to more flexible employees.
    To test employees for their person-organization fit, the organization has to define its values and visions beforehand.
    The evaluation can be done using the Q-sort technique.
    In general, the influence of both person-environment fit types is essential in employee selection.
    \item \textbf{Descriptive approach}\\
    The descriptive approach focuses on how these guidelines play out in selection processes.
    Even though traditional selection research focused on the person-job fit, the person-organization fit was already considered in the selection process.
    In most cases, this happened through applicant interviews, which showed surprisingly high validity when assessing person-organization fit.
    Also, the person-job fit was assessed in interviews, 
\end{itemize}
If those two concepts diverge, it may be because the research outcomes need to be appropriately communicated or required to be followed in practice.
Furthermore, there can also be some unexplored factors in research that influence the selection process.

\section{Quantizing the Person-Environment Fit}

\subsection{Text Mining Approach} \label{text_mining_approach}
The article \textcite{text_mining_for_automatic_profiling} describes the necessity of automatically filtering candidates due to the probably very high number of applicants when job vacancies are opened using the internet.
Moreover, in this article, the authors have tested their approach using an anonymized dataset of a company's actual applicants consisting of over $40$ resumes in the \textit{PDF} format for each of the three job positions \textit{Data Developer}, \textit{Human Resource and Development} and \textit{Marketing}.
Text mining is getting increasingly popular due to the big data trend that is expanding into many fields and the fact that most companies probably have many unstructured text documents to analyze \parencite[49]{text_mining_for_automatic_profiling}.
Some use cases for text mining are sentiment analysis or classification, article classification, spam or fake news detection, argument extraction, exploring social issues, logs mining, search personalization, article summarization and automatic recommendation systems \parencite[49]{text_mining_for_automatic_profiling}.
The text-processing algorithm can extract skills, education, and experiences from unstructured resumes to summarize each application.
These extracted keywords are matched against a keyword dictionary of each job vacancy that was created based on the terms used in each profession by the human resource department of the company that provided the dataset \parencite[47]{text_mining_for_automatic_profiling}.
This is also called the profile matching process that compares the present competencies of the applicant with the required competencies of the job vacancy and tests the person-job fit regarding the demands-abilities perspective.
It is assumed that the higher the person-job fit, the higher the congruence between the demands and abilities.
This simple matching will not work as expected in all cases since it suffers from the extracted words losing their contextual information and structure, and the model also does not understand the inherent meaning of the words as matching is done solely by equality checks \parencite[517]{applicant_semantic_matching}.
The final ranking was then compared to the company's human resources department ranking, which provided the anonymized resume dataset.
There were two methods used to compare the extracted keywords against the keyword dictionary of each job vacancy \parencite[53-58]{text_mining_for_automatic_profiling}:
\begin{enumerate}
    \item \textbf{Document Vector Analysis}\\
    This analysis used the \textit{KNIME Analytics Platform} for document preprocessing and vectorization.
    The preprocessing steps on the \textit{PDF} documents include punctuation erasure, number filtering, small word removal with an $n$-char filter (removal of words of length smaller or equal to $n$), conversion to lowercase letters, stop word removal (removal of words not containing any information like \textit{the} and \textit{or}, these words are language-specific) and as final step the bag of words creation (method to build a list of all used words in all input documents).
    Then, document vectorization is applied, which converts the preprocessed data of each resume to a vector of length $N$, where $N$ is the number of unique words in the bag of words.
    The vector represents a one-hot-encoding, meaning that if one of the unique words is present in the processed resume, this one value out of $N$ will be $1$. Otherwise, it will be $0$.
    This sample vector representation is also computed for the keyword dictionary that was previously defined, whose words are also added to the bag of words.
    The closer the vector representations of the resumes are to the vector representation of the keyword dictionary concerning Euclidian distance, the higher the predicted person-job fit is.
    A downside of this approach is that the presence of words in resumes outside the predefined keyword dictionary will increase the Euclidian distance even if they may not hurt the person-job fit. 
    This could be tackled by restricting the bag of words to the words in the keyword dictionary.
    \item \textbf{N-Gram Analysis}\\
    In addition to the preprocessing steps described in the document vector analysis, the N-gram analysis also includes sentence tokenization (separation of sentences), word tokenization (separation of words), dictionary tagger (tag words that are in a specific language, for example, English) and term identification using verb phrase chunking (splits remaining phrases into its constituents).
    As the bag of words method has the downside of losing the word order and the meanings of word combinations, the data here is analyzed with the N-gram method.
    That means the predicted person-job fit is higher when a word combination of the manually crafted keyword dictionary is found in the processed resume as a $1$-, $2$- or $3$-gram, depending on the keyword word count.
    The more keywords that are found to be present as $N$-grams in the processed resume, the higher the predicted person-job fit is.
\end{enumerate}
This method suffers from the manual creation of a keyword dictionary, which could be automated. 
Furthermore, the results heavily depend on the quality and selection of the keywords in the keyword dictionary.
The accuracy of the $N$-gram model was slightly higher than the accuracy of the document vector model on average and topped at $87.5\%$ for the \textit{Data Developer} role.
Moreover, when analyzing text based on chunks of words, the context of the words may be lost.
This thesis tries to mitigate this shortcoming by using the capabilities of \acs{LLM}s to summarize the context of the input documents and answer questions regarding the provided context.

\subsection{Machine Learning Approach} \label{machine_learning_approach}

The article \textcite{pj_fit_ml} describes a machine learning approach to predict the person-job fit.
As also outlined by \textcite[1]{pj_fit_ml}, manual inspection of applicant documents from human resource workers is no longer feasible due to human judgment's subjective, incomplete, and inefficient nature.
The authors of this article have developed a novel end-to-end \gls{TAPJFNN} framework that should reduce human labor and make the employee selection process more interpretable.
The key idea is to exploit the information in historical job application data using a word-level semantic representation for job requirements and job seekers' experiences based on the Recurrent Neural Network architecture \parencite[1]{pj_fit_ml}.
There are also two hierarchical topic-based ability-aware attention strategies designed in this article to measure the different importance of job requirements for semantic representation, as well as measure the varying contribution of each job experience to a specific ability requirement \parencite[1]{pj_fit_ml}.
The \acs{TAPJFNN} framework supports predicting person-job fit, talent sourcing tasks, and job recommendation tasks \parencite[1]{pj_fit_ml}.
There has also never been a formal, mathematical definition of the person-job fit quantization task, but the article \textcite{pj_fit_ml} provides this definition \parencite[2]{pj_fit_ml}.
It is proposed in \textcite[2]{pj_fit_ml} that the job description is represented as a set of ability requirements.
The applicants' abilities are extracted from the applicants' documents. They are matched against the ability requirement set by using a weighted, accumulative score, as several abilities can cover each requirement, and most applicants will have different abilities. 
This approach should counteract the deficiencies of the text mining approach as described in section \ref{text_mining_approach}, where exact keywords or $N$-grams are matched, which can lead to ignoring some abilities if some abilities are not described using the expected keyword list or to misleading of the recruiters due to incomplete or subjective weightings used in the score construction.
Also, the constructed keyword features require manual labor, which is a significant downside \parencite[5]{pj_fit_ml}.
Therefore, \textcite[2]{pj_fit_ml} propose to use machine learning to weight abilities based on historic recruitment results instead of just using a semantic understanding of rich textual information.

\subsubsection{Person-Job Fit Quantization Problem Definition}
The person-job fit quantization problem is mathematically defined as follows in \textcite[7-8]{pj_fit_ml}:
\begin{itemize}
    \item $j_l$: $l^{th}$ job requirement in job posting $J$
    \item $r_l$: $l^{th}$ work/project experience in candidate's resume $R$
    \item $w^J_{l,t}$ work embedding of the $t^{th}$ word in job requirement $j_l$
    \item $w^R_{l,t}$ work embedding of the $t^{th}$ word in candidate's experience $r_l$
    \item $h^J_{l,t}$ word-level representation of $t^{th}$ word in job requirement $j_l$
    \item $h^R_{l,t}$ word-level representation of $t^{th}$ word in candidate's experience $r_l$
    \item $s^J_l$ single topic-based ability-aware representation of job requirement $j_l$
    \item $s^R_l$ single topic-based ability-aware representation of candidate's experience $r_l$
    \item $g^J$ multiple topic-based ability-aware representation of job posting $j$
    \item $g^R$ multiple topic-based ability-aware representation of candidate's resume $r$
    \item $g^{J,R}_{+,1:k}$ multiple topic-based ability-aware representations of $k$ candidates' resumes who successfully applied for the job $J$.
    \item $g^{J,R}_{-,1:k}$ multiple topic-based ability-aware representations of $k$ candidates' resumes who unsuccessfully apply for the job $J$.
    \item $p$ number of job requirements in the job posting $j$.
    \item $q$ number of work/project experiences in candidate's resume $r$.
    \item $m_l$ number of words in job requirement $j_l$.
    \item $n_l$ number of words in candidate's experience $r_l$.
\end{itemize}
The identifier $J$ denotes a job posting containing $p$ pieces of job requirements and duties called \textit{ability requirements}.
The job posting $J$ is the sum of all ability requirements, which means $J = \{j_1,j_2,...,j_p\}$.
\textcite[7]{pj_fit_ml} divide the ability requirements into two categories:
\begin{itemize}
    \item \textbf{Professional skill requirements}\\
    These requirements are related to the professional skills that are required to perform the job, like \textit{Data Mining}, \textit{Truck Driving}, or \textit{Hair Cutting}, for example.
    \item \textbf{Comprehensive quality skills}\\
    These requirements are related to the comprehensive quality skills that are required to perform the job, like \textit{Communication}, \textit{Team Work}, or \textit{Sincerity}, for example.
\end{itemize}
These two types are comprehensively analyzed without any distinction between them.
Moreover, each job ability requirement $j_l$ is assumed to contain $m_l$ words, that means $j_l = \{j_{l,1},j_{l,2},...,j_{l,m_l}\}$.
The identifier $R$ denotes a candidate's resume, which contains $q$ pieces of experience.
The resume $R$ is the sum of all experiences, which means $R = \{r_1,r_2,...,r_q\}$.
Furthermore, each candidate's experience $r_l$ is assumed to contain $n_l$ words, that means $r_l = \{r_{l,1},r_{l,2},...,r_{l,n_l}\}$.
The job application is denoted as $S$ and is represented by a pair of $J$ and $R$, that has an associated recruitment result label $y \in \{0,1\}$ where $y = 1$ means a successful application and $y = 0$ means a failed one.
Each job $J$ may have many different applicants $R$, and each applicant $R$ may apply for many different jobs $J$.
The mathematical problem definition can now be given as provided in \textcite[8]{pj_fit_ml}:
\begin{definition}
    Given a set of job applications $A$, where each application $S \in A$ contains a job posting $J$ and a resume $R$ as well as the corresponding recruitment result label $y$, the target of person-job fit is to learn a predictive model $M$ for measuring the matching degree between $J$ and $R$. Then, the corresponding result label $y$ can be predicted. 
\end{definition}

\subsubsection{\acs{TAPJFNN} Architecture}
The \acs{TAPJFNN} architecture is visualized in the following figure:
\begin{figure}[H]
    \centering
    \includegraphics[scale=1,page=8,width=0.8\linewidth,trim={50 435 50 80},clip]{literature/pj_fit_ml.pdf}
    \caption{\acs{TAPJFNN} architecture \parencite[8]{pj_fit_ml}}
    \label{fig:tapjfnn_architecture}
\end{figure}
Each job requirement $j_l$ is a paragraph in the job posting $J$.
Each experience $r_l$ is a paragraph in the candidate's resume $R$.
As can be seen in figure \ref{fig:tapjfnn_architecture}, the architecture mainly consists of the following three components \parencite[8-14]{pj_fit_ml}:
\begin{itemize}
    \item \textbf{Word-Level Representation}\\
        Both the job requirements and the applicant's experiences are encoded on a word basis to an internal embedding using a \textit{BiLSTM} model, which is constructed out of two \textit{LSTM} models, one that gets the input step-by-step in the forward direction (from first to last) and one that receives the input step by step in the backward direction (from last to first).
        The outputs of both \textit{LSTM} models are concatenated to form the final output of each step.
        Each word in a job requirement $j_l$ or an applicant's experience $r_l$ is encoded to a word embedding $w^J_{l,t}$ or $w^R_{l,t}$ respectively by using the same pre-trained word vector matrix $W_e$ that is retrained during training and the following equation $w^J_{l,t} = W_e \cdot j_{l,t}$ or $w^R_{l,t} = W_e \cdot r_{l,t}$ respectively.
        The equation to compute the word-level representation $h^J_{l,t}$ or $h^R_{l,t}$ respectively is $h^J_{l,t} = BiLSTM(w^J_{l,1:m_l},t) \; \forall t \in [1,...,m_l]$ or $h^R_{l,t} = BiLSTM(w^R_{l,1:n_l},t) \; \forall t \in [1,...,n_l]$ respectively.
        These equations have the disadvantage that the words at the beginning have less context than the words at the end, as the first word has just access to the first and last word to compute its word-level representation.
    \item \textbf{Hierarchical Topic-Based Ability-Aware Representation}\\
    This component is divided into the following four parts:
    \begin{itemize}
        \item \textbf{Single Ability-Aware Part for Job Requirement}\\
        This layer is responsible for summing up the attention-weighted intermediate word-level representations $h^J_{l,t}$ to a single ability-aware requirement representation $s^J_l$ for each job requirement $j_l$.
        Some words might be more important than others in a job requirement. Therefore, the attention mechanism is used to weigh the importance of each word in a job requirement and shift the weighting towards keywords.
        This summing can be donated as $s^J_l = \sum_{t=1}^{m_l} \alpha_{l,t} \cdot h^J_{l,t}$ where $\alpha_{l,t} = \frac{\exp(e^J_{l,t})}{\sum_{i=1}^{m_l} \exp(e^J_{l,i})}$ is the attention weight of the $t^{th}$ word in job requirement $j_l$.
        The term $e^J_{l,t}$ is defined to be $v_{\alpha}^{\intercal} \tanh(W_{\alpha} \cdot h^J_{l,t} + b_{\alpha})$.
        All parameters that are not described are initialized and learned in the training process.
        \item \textbf{Multiple Topic-Based Ability-Aware Part for Job Requirement}\\
        The single ability-aware part for job requirement is extended to the multiple topic-based ability-aware part for job requirement by summing the weighted results from the previous layer according to $g^J = \sum_{t=1}^p \beta_t c_t^J$.
        The expression $c_t^J$ can be computed as $c_t^J = BiLSTM(s^J_{1:p},t) \; \forall t \in [1,...,p]$.
        The term $\beta_t$ is defined to be $\frac{\exp(f^J_t)}{\sum_{i=1}^p \exp(f^J_i)}$ where $f^J_t = v_{\beta}^{\intercal} \tanh(W_{\beta} \cdot c_t^J + U_\beta \cdot z^J + b_{\beta})$.
        The term $g^J$ is the final representation of the job requirements of job posting $J$, which contains the summed-up requirements weighted with their importance.
        All parameters that are not described are initialized and learned in the training process.
        \item \textbf{Single Ability-Aware Part for Experience}\\
        The word-level representations $h^R_{l,t}$ for each job experience $r_l$ are summed up for each job requirement $j_k$ to a single ability-aware experience representation $s^R_{l,k}$ which equals $\sum_{t=1}^{n_l} \gamma_{l,k,t} \cdot h^R_{l,t}$.
        The term $\gamma_{l,k,t}$ is defined to be $\frac{\exp(e^R_{l,k,t})}{\sum_{i=1}^{n_l} \sum^p_{j=1} \exp(e^R_{j,k,i})}$ where $e^R_{j,k,i} = v_{\gamma}^{\intercal} \tanh(W_{\gamma} \cdot s^J_{k} + U_\gamma \cdot h^R_{l,r} + b_{\gamma})$.
        This is defined as a novel approach as the attention is not only applied to one dimension, which would be all experiences comparably to the job requirements case but the second dimension of the job requirements is also considered. 
        Moreover, the attention mechanism uses $s^J_k$ as input to correctly weigh the words in the experiences regarding the requirement $j_k$.
        All parameters that are not described are initialized and learned in the training process.
        \item \textbf{Multiple Topic-Based Ability-Aware Part for Experience}\\
        In this layer, the first step is to sum up the attention-weighted intermediate representations $s^R_{l,k}$ to a single ability-aware experience representation $s^R_l$ for each job experience $r_l$.
        This is done by using the following equation $s^R_l = \sum_{t=1}^p s^R_{l,t}$. 
        These semantic representations are again accumulated with a \textit{BiLSTM} to extract temporal relationships between them.
        These accumulated representations are computed as $c_t^R = BiLSTM(s^R_{1:q},t) \; \forall t \in [1,...,q]$.
        The final representation of the job experiences of resume $R$ is computed as $g^R = \sum_{t=1}^q \delta_t c_t^R$ where $\delta_t = \frac{\exp(f^R_t)}{\sum_{i=1}^q \exp(f^R_i)}$ and $f^R_t = v_{\delta}^{\intercal} \tanh(W_{\delta} \cdot g^J + U_\delta \cdot c^R_t + M_\delta \cdot z^J + V_\delta \cdot z^R + b_{\delta})$ which represents the attention mechanism in this architecture part.
        All parameters that are not described are initialized and learned in the training process.
    \end{itemize}
    \item \textbf{Person-Job Fit Prediction}\\
    The final prediction result can now be computed as $\hat{y} = sigmoid(W_y \cdot D + b_y)$ where $D$ is defined as $tanh(W_d[g^J;g^R;g^J-g^R;g^J \odot g^R;z^J;z^R;(W_Jz^J + b_J) \odot (W_Rz^R + b_R)] + b_d)$.
    All parameters that are not described are initialized and learned in the training process.
    The application is predicted to succeed if the $\hat{y}$ is bigger than $0.5$. Otherwise, it is predicted to be unsuccessful.
    The researchers have also implemented a refined prediction strategy that improves the model's accuracy by supplying the model additionally with $k$ successful and $k$ unsuccessful applications for the same job posting $J$.
    This is not presented here and can be found in \textcite[13-14]{pj_fit_ml}.
\end{itemize}
The accuracy of the \acs{TAPJFNN} architecture was tested on a large dataset of around $10000$ applicants, and the resulting accuracy was around $85\%$.
Of course, the disadvantage of this approach is that the model is only as good as the data it is trained on. Many successful and unsuccessful applications are needed for the training, making this method infeasible for small companies.
The researchers discuss in their article that the attention mechanisms make the whole architecture more interpretable as the requirements, experiences, or words where the model's attention scores are high identify the focus of the underlying model.
The discussion was more detailed here because the authors successfully applied the divide and conquer strategy to the person-job fit quantization problem and tried to remodel the human steps as differentiable functions in their \acs{TAPJFNN} architecture.

\subsection{Mixed Approach}
The model introduced in \textcite[517]{applicant_semantic_matching} uses a mixed approach to match applicants to job postings by incorporating a combination of supervised learning algorithms and a semantic matching system.
The evaluation of the applicants in this system is based on objective criteria which are directly extracted from an applicant's \textit{LinkedIn} profile, and the personality characteristics are deduced from the applicant's social presence on \textit{LinkedIn} \parencite[517]{applicant_semantic_matching}.
The $5$ objective criteria that were chosen were \parencite[518]{applicant_semantic_matching}:
\begin{enumerate}
    \item Education (years of formal academic training)
    \item Work Experience (months of related experience)
    \item Loyalty (average number of years spent per job)
    \item Extraversion (deduced from the applicant's social presence)
    \item Skills (used to be semantically matched to the job requirements)
\end{enumerate}
It was also introduced as a semantic matching system to match the applicant's past experiences to the job requirements. It also incorporated an algorithm that determined whether the past work experience was within the domain of expertise of the job position.

The system consisted of the following $4$ components \parencite[518-519]{applicant_semantic_matching}:
\begin{itemize}
    \item \textbf{Semantic Matching}\\
    This semantic matching system calculates the semantic distance between candidate skills or prior experiences and the job requirements \parencite[518]{applicant_semantic_matching}.
    Semantic matching means that annotations using controlled domain-specific vocabularies are matched with background knowledge about a particular application domain, which is modeled as a taxonomy of skills in this domain that was created by an expert \parencite[519]{applicant_semantic_matching}.
    A taxonomy is a set of categories or terms organized into a hierarchy with parent-child relationships and implied inheritance. More general concepts are on the top, and more specific concepts are on the bottom, as shown in the following picture \ref{fig:it_skill_taxonomy}:
    \begin{figure}[H]
        \centering
        \includegraphics[scale=1,page=5,width=0.8\linewidth,trim={45 510 45 55},clip]{literature/applicant_semantic_matching.pdf}
        \caption{A part of an IT skill taxonomy \parencite[519]{applicant_semantic_matching}}
        \label{fig:it_skill_taxonomy}
    \end{figure}
    Today, this taxonomy is labeled as a knowledge graph, a way to store information in graphs structurally.
    The taxonomy is used to match the skills mentioned in the applicant's profile (skill keywords are used), the applicant's current work experience (skills from unstructured text are used) and eventually (if the candidate has the required skills) the applicant' past work experiences (skills from unstructured text are used) to the job position requirements (skill keywords are used), whenever a required skill is absent the applicant is rejected and not included into the final ranking \parencite[519-520]{applicant_semantic_matching}.
    The months of work experience are only calculated for experiences that concern relative competencies \parencite[519]{applicant_semantic_matching}.
    As the required skills of the job and the skills of the user were provided as skill keywords, the problem of extracting skills from unstructured text was reduced to the work experiences.
    The search was not keyword-based but concept-based, that means because of the provided ontology, the system would know that a requirement of \textit{structured} can be fulfilled by the skill \textit{C} as can be seen in figure \ref{fig:it_skill_taxonomy}.
    The semantic distance of the required and present skills is based on the node distance metric \parencite[520]{applicant_semantic_matching}.
    \item \textbf{Personality Mining Module}\\
    This module uses the \textit{LinkedIn} user's posts to apply linguistic analysis and derive features that may reflect the user's personality, which is often overlooked in existing recruitment pipelines \parencite[518-520]{applicant_semantic_matching}.
    It was scientifically verified that by using the \acs{LIWC} system, the frequency of certain words in written texts correlated with the \textit{Big-Five} personality dimensions, but the dimension extraversion received the most research attention in this regard and was the dimension exclusively used in this article \parencite[521]{applicant_semantic_matching}.
    \item \textbf{Job Application Module}\\
    This module implemented forms necessary to fill out if a candidate wanted to apply to a job \parencite[518]{applicant_semantic_matching}.
    \item \textbf{Applicant Ranking Module}\\
    This module combines the candidate's selection criteria to derive the candidate's relevance score for the applied position using a parameterized function derived through supervised learning algorithms \parencite[518]{applicant_semantic_matching}.
    As is the case with all machine learning techniques, this one, too, requires sufficient training data of past candidate selection decisions, which will only be present in the necessary scale in some companies \parencite[523]{applicant_semantic_matching}.
    This also raises the problem that the model will learn the past human bias from past candidate selections, as was the case in \textcite[9]{bias_ai_hiring}.
    Of course, all past candidates must again be used to compute the $5$ objective criteria. Also, a human expert recruiter must annotate all the past recruitment decisions with relevant scores that the model should learn. This involves a lot of manual labor \parencite[523-524]{applicant_semantic_matching}.
\end{itemize}

\section{Bias in \acs{AI}-enabled Recruiting Tools}
The article \textcite[9]{bias_ai_hiring} mentions that even \textit{Amazon} had an \acs{AI}-enabled hiring tool that had an internal bias towards male candidates while penalizing female applicants.
Bias in this context means that identical applications apart from the name and sex would be ranked differently by this algorithm, even if they should be ranked equally.
As mentioned in \textcite[9]{bias_ai_hiring}, the problem was that since the machine learning model was trained with historic recruitment data, it also inherently learned the bias already present in this historical data.
This was the bias from the past decisions of human recruiters that seemed to prefer male applicants over female ones, and past choices also penalized African Americans \parencite[9-10]{bias_ai_hiring}.
However, several startups want to show that with a carefully designed system, the human bias can also be removed from the recruitment process \parencite[9]{bias_ai_hiring}.
Some techniques these startups are using are extracting identifying information from applications, holding anonymous interviews and skill-set tests, and tuning the job posting wording to attract a more diverse set of applicants \parencite[9]{bias_ai_hiring}.
The first approach is the one that this thesis takes, the second approach eliminates gender bias by focusing on the skills and impartiality of all candidates (personally identifiable information is stripped away), and the third approach is not applicable in this thesis as the job postings have already been created and are input to the model.
This shows that a bias-free recruiting process cannot be achieved by just replacing one tool in the pipeline. All tools must be carefully designed to be bias-free, starting from the job description wording and ending with a valid way to assess the candidate's skills to make an informed decision.
The second approach can be used to assess the candidates further, which the proposed model from the thesis finds promising, and the third approach is also necessary to attract a diverse set of applicants in the first place.
A filtering model like the one from the thesis can never reintroduce diversity that is not present among the applicants, so the wording of the job description is crucial.
Another essential aspect that the article \textcite[9-10]{bias_ai_hiring} mentions is that (continually) trained models must also be monitored for bias continuously, and the recruiters must know that the results from \acs{AI}-enabled tools cannot be trusted blindly but must be overridden by the human judgment if necessary.
Furthermore, constantly staying on alert and cross-checking the model results with human judgment might defeat some of the efficiency gains the model may bring.
The bias issue is increasingly important if the \acs{AI}-enabled tools are increasingly used as autonomous and not as assistive tools, which means the tools make active hiring decisions instead of just providing suggestions or recommendations within the process.
Moreover, there was also a software presented in \textcite[10-11]{bias_ai_hiring} that computes a match score between a candidate's documents and a job's requirements similar to the model proposed in this thesis, and it was reported that with this matching system, recruiters were more likely to consider underprivileged and underrepresented minorities to move forward in the hiring process, significantly improving the overall hiring diversity.
There is also the issue with verifying the claims of \acs{AI}-assisted tools as there currently is no publicly available dataset actually to benchmark these systems \parencite[11]{bias_ai_hiring}. That issue also made the creation of original sample applicant data necessary for this thesis \parencite[11]{bias_ai_hiring}.
Companies do not want to share their training data as this can lead to data privacy or liability issues if some bias is found in the data or the model, so the current system, lacking robust mechanisms for verification and inherently susceptible to confirmation biases, must undergo a radical and scientifically-driven transformation in the future \parencite[11]{bias_ai_hiring}.

\section{\acs{AI}-enabled Recruiting as Marketing Tool and Current Developments}
The article \textcite[215]{marketing_ai_recruitment} mentions that companies do not need to spend money to hide their use of \acs{AI} in the application process. Instead, they can use it to promote their use, which significantly positively affects job application likelihood due to its novelty factor if the candidate already has a positive view of the organization.
\acs{AI} can utilize physiological characteristics (face recognition, DNA, hand geometry, iris recognition, micro-expressions, scent, and retina scanning) and behavioral traits (gait, typing rhythm, and voice patterns) as part of the candidate selection process apart from the aforementioned text-based features of a candidate \parencite[215]{marketing_ai_recruitment}.
The research testing the validity of these machine learning-based methods that try to infer and extrapolate characteristics in terms of job fit and performance is lagging and gives rise to data privacy and bias discussions as it is also possible to deduce a person's sexual orientation from a face recognition with remarkable accuracy \parencite[215]{marketing_ai_recruitment}.
It is essential to transparently communicate what can be done with the current state-of-the-art technologies, but the question of what should be done must be asked from an ethical perspective.
The more data is collected, the more seemingly unrelated features of this data can be used to infer more characteristics about the candidate with reasonable accuracy, but since those characteristics are extracted using machine learning methods, they are not valid in any particular applicant's case which gives rise to ethical and privacy concerns \parencite[215]{marketing_ai_recruitment}.
Furthermore, \acs{AI}-enabled recruiting tools also have the potential to be more efficient and effective than human recruiters, and their use within e-recruitment tools is positively catalyzed with an increased technology use motivation of job seekers due to its perceived usefulness \parencite[216]{marketing_ai_recruitment}.
The candidate's attitude towards the brand or image of the organization is one of the critical factors that prevent applicants from applying for a job. Still, with the help of \acs{AI} and its novelty factor, the technology use motivation can be increased, leading to a higher job application likelihood \parencite[217]{marketing_ai_recruitment}.
The article also mentions that an applicant's anxiety towards the use of \acs{AI} in recruitment is secondary to the applicant's attitude towards the hiring organization \parencite[217]{marketing_ai_recruitment}.
Candidates are more satisfied with technology-based recruitment when it is technologically advanced and user-friendly and if the recruitment has a high perceived efficiency \parencite[219]{marketing_ai_recruitment}.
The novelty of \acs{AI} fosters a positive, sustainable pre-employment relationship behavior where ease of use and playfulness enable candidates with low cow cognitive innovation to form sustainable relationships with potential employers. At the same time, aesthetics, service excellence, and usefulness are the enablers for candidates with high cognitive innovation \parencite[220]{marketing_ai_recruitment}.

\chapter{Methodology} \label{methodology}

\section{Discussion on the Methods}
Qualitative methods are concerned with describing, interpreting and understanding contexts, establishing classifications or typologies and generating hypotheses. 
In the thesis' case the qualitative consideration of the object of investigation is backed by an extensive literature review in chapter \ref{theoretical_background}.
The literature review focuses on the wide application area and superior generality of \acs{LLM}s and how other researchers have approached the person-environment fit quantization problem.
As qualitative methods should be used when new findings or new theories need to be developed, these methods are a good fit for the thesis' case as the \acs{LLM} approach to the person-environment fit quantization problem is a relatively novel approach with few available literature.
Those findings and theories cannot be generalized in most cases, as they mostly explore particular cases in depth, like the \acs{LLM} approach in our case. 
As described in \textcite[127]{qualitative_methods} theories are constructed from the results and conclusions are drawn for practice.
The qualitative methods should lead to a conclusion whether an \acs{LLM}-based model is a good fit for the person-environment fit quantization problem and what human expert recruiters are expecting from such assistive technologies including their considerations.
Furthermore, also recruiters' past experiences with such assistive technologies should be part of the results.
The used qualitative methods should enforce openness, breadth, detail, proximity and interdisciplinarity in favor for representativeness and structuring \parencite[127]{qualitative_methods}.
Quantitative methods, on the other hand, are concerned with the most accurate description and predictability of behavior in the form of models, correlations and numerical characteristics, i.e. the measurement of quantities, proportions or numbers of one or more specific characteristics of the object under investigation.
These are also needed in the thesis' case, as the subordinate research questions can only be answered by measuring the proposed model's accuracy against human expert recruiters while tracking the associated time usages.
Quantitative methods will therefore be used to confirm or refute the theory that \acs{LLM}-based models might be a good fit for the person-environment fit quantization problem.
The results from quantitative methods may generalize well if statistical significance is given within the results.
This brief discussion on qualitative and quantitative led to the result that in the context of the thesis it is beneficial that a hybrid approach is used (qualitative and quantitative methods are used) to answer the research question.
As less is known about the object of investigation, a qualitative literature review is necessary before going forward with quantitative methods. 
From the processing and interpretation of verbal data from the qualitative methods, necessary features can be derived for the proposed model's implementation.

\subsection{Discussion on the Qualitative Methods}
These are the available survey instruments for qualitative research:
\begin{itemize}
    \item \textbf{Narrative Interview}\\
    The advantage with this kind of survey instrument might be that the interviewed entity is not particularly constrained in their answers and can tell their story in their own length, expansiveness and level of detail as questions are very open-ended.
    A disadvantage might be that certain areas will not be covered in the expected level of detail and some areas will not be covered at all if the interview is not guided at all.
    \item \textbf{Guideline Interview}\\
    The disadvantage with this kind of survey instrument might be that the interviewed entity is somewhat constrained be the more specific questions of the interviewer that means the answers will be less diverse and more specific to the question that was asked than in a narrative interview.
    An advantage might be that all expected areas will be covered in the expected level of detail if the interview is guided well.
    \item \textbf{Group Discussion}\\
    Group discussions might have the advantage that participants can interact with each other and build on each other's ideas. 
    That means that outputs might be more diverse and richer than in subsequent one-on-one interviews.
    A disadvantage might be that more introverted people may be overshadowed by more extroverted people and that the discussion might be dominated by a few participants.
    Furthermore, some participants may not want to or are allowed to share internal business-critical company information in a group setting.
    \item \textbf{Text Analysis}\\
    This survey instrument can be used to do a literature review in order to get a qualitative understanding of the object of investigation and to get new research and model implementation ideas from ongoing research.
    The advantage is that lots of information is easily accessible which means that a good theoretical foundation can be built fast.
    A disadvantage may be that literature regarding certain areas may not be available in the expected level of detail and some areas may not be available at all.
    \item \textbf{Observation Analysis}\\
    This survey instrument is not suitable as it is mostly used for social studies where observation refers to all forms of sensory perception.
    As our object of investigation is a computer program, it makes no sense to observe it in the traditional sense, as it will only execute the way it is programmed to.
\end{itemize}
By analyzing all the discussed survey instruments, it was decided that the most suitable survey instrument for the thesis' case is the guideline interview in a group discussion setting combined with an extensive literature review (text analysis).
The decision was taken because the group discussion will help to get a diverse set of opinions and may reveal points that have been forgotten in the interview guidelines and the guideline interview will help to guide the discussion to the expected areas that should be covered in the expected level of detail.
These areas include the expectations and considerations on assistive technologies in the \acs{HR} pipeline and the experiences with \acs{AI}-assisted technologies in the context of \acs{HR}.
Narrative interviews are not used as very specific areas need to be addressed in the discussion and in a group setting, the time of all participants needs to be respected.
As mentioned before, the observation analysis makes no sense in this thesis' case.

\subsection{Discussion on the Quantitative Methods}
These are the available survey instruments for qualitative research:
\begin{itemize}
    \item \textbf{Survey}\\
    adsf
    \item \textbf{Standardized Observation}\\
    adsf
    \item \textbf{Experiment}\\
    adsf
    \item \textbf{Panel}\\
    adsf
    \item \textbf{Secondary Data}\\
    adsf
\end{itemize}

\section{Qualitative Research Design} \label{qualitative_research_design}
A focus group of five human expert recruiters will be assembled to analyze the model's output and impact from a qualitative perspective.
This amount of experts was chosen to have diverse opinions, but only a few participants, to reduce the coordination work.
Those will be involved in three major interactions:
\begin{enumerate}
    \item \textbf{Kick-Off Meeting} \label{kick_off_meeting}\\ 
    In this meeting, the human expert recruiters will be introduced to the proposed model, and the topic of machine learning-assisted technologies in human resources will be discussed following these major topics to collect the requirements of the domain:
    \begin{enumerate}
        \item \textbf{Bias and Fairness}\\
        This topic entails discussing bias and fairness in the context of machine learning-assisted technologies in human resources.
        Furthermore, the technologies should probably behave in a way that avoids discrimination based on gender, ethnicity, age, or other non-job-related factors.
        Moreover, more general ethical considerations should also be discussed.
        \item \textbf{Accuracy and Reliability}\\
        This topic entails a discussion of how well the technologies should be able to replicate human judgment.
        The operation should probably be reliable, and different \acs{CV} formats and layouts should be supported while keeping the ability to match job requirements with various qualifications and experiences in other industry fields.
        \item \textbf{Legal Considerations}\\
        This topic entails discussing the legal considerations of the technologies that must be matched to use \acs{AI}-assisted technologies in real-world human resource processes.
        This includes the discussion of data security and privacy, as well as the debate on the transparency of the technologies.
        \item \textbf{Machine Learning as Assistive Technology}\\
        This topic entails a discussion on the role of machine learning in the human resource process and how it can complement the human decision-making process.
        There can maybe also be a discussion on scenarios where it would be better not to use \acs{AI}-assisted technologies or a debate on the balance of automated and human decision-making in the screening process and whether the proposed architecture matches the expected balance or if changes should be implemented.
        Moreover, the expected efficiency gains of the technologies should be discussed.
        \item \textbf{Customization and Adaptability}\\
        This topic entails discussing the customization and adaptability of the technologies and whether they can adapt to specific industry needs, company cultures, and varying contexts.
        Moreover, there must be things that the users might want to tweak to customize the technologies to their needs.
        For example, this can be the weighting or definition of certain job requirement types or general hints on matching the job requirement to the applicant's screening documents.
        It is essential to understand how important it is for users to feed their improvement suggestions into the system.
        This could be done so every user can tweak the system's configuration.
        \item \textbf{User Experience}\\
        This topic entails a discussion on the user experience of the technologies, whether they are easy to use, understand, and integrate into existing systems.
        Moreover, the file types and programs used in current human resource processes should be discussed here.
        Furthermore, it is essential to know if these programs support third-party export and import functionalities in order to use an external program for the screening task.
        \item \textbf{Candidate experience}\\
        This topic entails a discussion on the candidate's experience of the technologies and whether they have any advantages on their side.
        Moreover, these advantages may include a faster response time and automated feedback generation for positive and negative feedback.
        \item \textbf{Previous experiences with \acs{AI}-assisted technologies in human resources}\\
        This topic entails discussing the human experts' previous experiences with \acs{AI}-assisted technologies in the context of human resources.
        Moreover, the discussion should include these technologies' general reception, advantages, disadvantages, potential improvements and drawbacks, and their field of use within the human resource pipeline.
        \item \textbf{Costs}
        This topic entails a discussion on the costs of the technologies, whether they are affordable, and whether they are worth the investment, including the expected costs for a model as outlined in this thesis. 
    \end{enumerate}
    \item \textbf{Ranking Announcement} \label{ranking_announcement}\\
    After the kick-off meeting, each human expert recruiter will get a set of applicant documents and job descriptions that should be ranked and categorized in \ref{quantitative_research_design}.
    The final outputs of the experts and the time it took to produce the outputs per job description will be collected via an online form that will be provided for that purpose.
    \item \textbf{Closure Meeting} \label{closure_meeting}\\ 
    In this meeting, the human expert recruiters will get the model's output to the same applicant documents and job descriptions that the recruiters have gotten.
    The model's output will be provided to the recruiters beforehand so they can prepare for the meeting.
    The model's output will be compared to the human expert recruiters' outputs, and the differences or similarities will be discussed.
    There will also be user tests of the model where the time taken to interface with the model will be measured.
    These topics will guide the discussion with the experts:
    \begin{enumerate} 
        \item \textbf{Consistency and Agreement}\\
        This topic entails discussing the consistency and agreement of the model's output with the human expert recruiters' output.
        There, the satisfaction of the achieved accuracy on the categorization and the ordering task should be discussed, and potential model bias or potential outliers and their reasons can be analyzed.
        \item \textbf{Explainability and Transparency}\\
        To achieve a high degree of user confidence and trust, the users must understand how the model makes its decisions, and the model must have a way to communicate its decisions in a transparent and human-understandable way.
        There should be a discussion on the explainability and transparency of the model's output to the human expert users.
        If deficiencies in the model are found by inspecting the model's explanations, a discussion on tweaking the model's configuration should be held, as well as whether it is understandable and easy to use.
    \end{enumerate}
\end{enumerate}
All the information gathered will be qualitatively analyzed, and the discussions will be summarized while presenting the considerations of the recruiters' common sense.
Possible companies that could participate in the qualitative research part of this thesis:
\begin{enumerate}
    \item \textbf{Michael Page International Austria GmbH}\\
    phone: +4312052050, mail: contact@michaelpage.at
    \item \textbf{Randstad Austria GmbH}\\
    phone: +43152455010, mail: info@randstad.at
    \item \textbf{Hays Österreich GmbH}\\
    phone: +4315353443260, mail: katharina.strass@hays.at
    \item \textbf{Markus Baldauf Personalberatung}\\
    phone: +436609991020, mail: markus.baldauf@mbmc.at
    \item \textbf{epunkt GmbH}\\
    phone: +43732611221, mail: daniel.petri@epunkt.com
    \item \textbf{Talentor Austria GmbH}\\
    phone: +4366460405835, mail: antonia.wild@talentor.com
    \item \textbf{Manpower Personalberatung Österreich}\\
    phone: +431516767000, mail: office@manpower.at
    \item \textbf{H \& F Personalmanagement GmbH}\\
    phone: +436642531902, mail: zentrale@hf-personalmanagement.com
    \item \textbf{Eblinger \& Partner Personal- und Managementberatungs GmbH}\\
    phone: +431532333330, mail: florens.eblinger@eblinger.at
    \item \textbf{Mondial-Recruiting e.U.}\\
    phone: +436643965221, mail: jobs@mondial-recruiting.com
    \item \textbf{Brunel Austria GmbH}\\
    phone: tel:+4366283000110, mail: salzburg.at@brunel.net
    \item \textbf{Arbeitsmarktservice Österreich}\\
    phone: +4350904900610, mail: martin.schoenbrunn@ams.at
    \item \textbf{Cadre e.U.}\\
    phone: +436505801643, mail: office@cadre.at
    \item \textbf{ISG Personalmanagement GmbH}\\
    phone: +4989954586880, mail: saba.gurleyen@isg.com
    \item \textbf{HR Consulting Alexander Wozak GmbH}\\
    phone: +4318771392, mail: alexander.wozak@hrconsulting.at
\end{enumerate}

\section{Quantitative Research Design} \label{quantitative_research_design}
The five human experts of the focus group introduced in \ref{qualitative_research_design} will get the same sample screening documents consisting of applicant documents and job descriptions.
The applicant documents should be ranked from best to worst applicant and categorized as promising and unpromising candidates per the job description by each human expert.
That means that for each job description, there is an applicant ranking from best (1\textsuperscript{st} place) to worst (8\textsuperscript{th} place) applicant, and there are also two applicant groups per job description, the promising applicant group and the unpromising applicant group.
The screening documents comprise six job descriptions of various industries with eight applicant document sets each.
This split was chosen to have diverse job descriptions with different applicants competing.
Furthermore, it is essential to note that recruiters must rank and categorize $48$ applicants while tracking their time usage. Therefore, there is quite some work to do on their end.
More importantly, the sample screening documents must also be provided in adequate quality to mimic the real-world screening process as closely as possible.
Al job descriptions will only have \acs{CV}s as applicant documents.
The length of each \acs{CV} is bound to six pages.
No applicant document will contain a photo of the applicant.
The following is a table summarizing the structure of the sample screening documents:
\begin{table}[H]
    \begin{tabular}{|l|l|l|l|}
    \hline
    \textbf{index} & \textbf{job description present} & \textbf{\acs{CV} present} & \textbf{cover letter present} \\ \hline
    1 & yes & yes & no \\ \hline
    2 & yes & yes & no \\ \hline
    3 & yes & yes & no \\ \hline
    4 & yes & yes & no \\ \hline
    5 & yes & yes & no \\ \hline
    6 & yes & yes & no \\ \hline
    \end{tabular}
    \caption{Structure of the sample screening documents}
\end{table}

The accuracy of the designed screening model will be determined by comparing the human expert annotations to the model's output for the sample screening documents and the following labels:
\begin{enumerate}
    \item \textbf{Ranking}\\
    The ranking of the model's output will be compared to the mean human expert ranking and the individual human expert rankings per job description.
    Metrics summarizing the ranking accuracy for all job descriptions will also be provided.
    The comparison of the rankings of the model against the mean or individual rankings of the human experts will be made using the rank-biased overlap metric, which was introduced in \textcite{rank_biased_overlap}. 
    The mean rankings and the summarized metrics will be computed with an arithmetic mean.
    Additional criteria will be introduced to break eventually occurring ties.
    \item \textbf{Categorization}\\
    The set of promising candidates of the model's output will be compared to the mean human expert categorization and the individual human expert categorizations per job description.
    Metrics that summarize the categorization accuracy for all job descriptions will also be provided.
    The comparison of the categorizations of the model against the mean or individual categorizations of the human experts will be done using the Hamming distance.
    Therefore, this would equal the number of applicant category changes necessary to transform one categorization into another.
    The mean categorizations will be computed using the mode, and the summarized metrics will be calculated with an arithmetic mean.
    Additional criteria will be introduced to break eventually occurring ties.
\end{enumerate}

The time savings associated with the model usage will be determined by sampling actual users using the model to automate the screening process.
These samples will be taken with the human expert recruiters at the closure meeting.

\subsection{Rank-Biased Overlap}
Rank-biased overlap (RBO) was used in \textcite{rank_biased_overlap} to compare the search results of various search engines.
It computes the similarity of two rankings from $0$ (no similarity) to $1$ (most similarity). It can be evaluated up to a certain depth $k$ and needs a weighting parameter $p$ out of the interval $(0,1)$.
The lower $p$ is chosen, the more weight is put on the top ranks of the rankings.
Here are some examples to illustrate the behavior of rank-biased overlap denoted as $rbo$, $k=\infty$, which means that the evaluation depth is as deep as the length of the ranking:
\begin{itemize}
    \item $rbo_{k=\infty,p=0.1}(['dog','cat','mom'],['dog','cat','mom']) \approx 1$
    \item $rbo_{k=\infty,p=0.1}(['apple','peach','mom'],['dog','cat','dad']) = 0$
    \item $rbo_{k=\infty,p=0.1}(['dog','cat','mom'],['dog','cat','dad']) \approx 0.99$
    \item $rbo_{k=\infty,p=0.6}(['dog','cat','mom'],['dog','cat','dad']) \approx 0.74$
    \item $rbo_{k=\infty,p=0.6}(['dog','cat','mom','tennis'],['dog','cat','dad','tennis']) \approx 0.80$
    \item $rbo_{k=\infty,p=0.6}(['dog','cat','mom','tennis'],['dog','cat','dad','golf']) \approx 0.78$
\end{itemize}
As described in \textcite[1]{rank_biased_overlap}, it was the first metric at that time that had the following three properties when comparing incomplete rankings (incomplete means that not every element from the population must be necessarily ranked):
\begin{enumerate}
\item Non-Conjointness (incomplete rankings must not necessarily contain the same elements from the population)
\begin{itemize}
    \item $rbo_{k=\infty,p=0.1}(['apple','peach','mom'],['dog','cat','dad']) = 0$
\end{itemize}
\item Weighting (the metric should weight high ranks more than low ranks, e.g., it is worse to have a difference in the 1\textsuperscript{st} place than in the 7\textsuperscript{th} place)
\begin{itemize}
    \item $rbo_{k=\infty,p=0.1}(['dog','cat','mom'],['dog','cat','dad']) \approx 0.99$
    \item $rbo_{k=\infty,p=0.6}(['dog','cat','mom'],['dog','cat','dad']) \approx 0.74$
\end{itemize}
\item Monotonicity (the ranking should be monotonic with increasing depth of evaluation as also indefinite rankings are supported, which means that the rank-biased overlap measure is non-decreasing with increasing depth of evaluation)
\begin{itemize}
    \item $rbo_{k=\infty,p=0.6}(['dog','cat','mom'],['dog','cat','dad']) \approx 0.74$
    \item $rbo_{k=\infty,p=0.6}(['dog','cat','mom','tennis'],['dog','cat','dad','tennis']) \approx 0.80$
    \item $rbo_{k=\infty,p=0.6}(['dog','cat','mom','tennis'],['dog','cat','dad','golf']) \approx 0.78$
\end{itemize}
\end{enumerate}
These properties were helpful when comparing search results from search engines as they most likely supply incomplete rankings, which are non-conjoint, massive in size (monotonicity and evaluation at a certain depth are desirable), and results at the front are much more important than search results further back in the ranking.
For this thesis, the second weighting property is the most important one, as we have complete, definite rankings that are conjoint.

\section{Sample Screening Documents} \label{sample_screening_documents}

\subsection{Overview of the Construction and Usage of the Sample Screening Documents}
The job descriptions and the applicant documents will be either taken from publicly available datasets, taken from publicly available sources, manually crafted, or generated using tools like \textit{GPT-4} \parencite{gpt4} while giving the model some hints on the context and the expected applicant characteristics.
Due to their internal tokenization, the job description and the applicants' \acs{CV}s must be supplied as pure text to the proposed model.
That means the more tools belonging to Digital Recruiting 3.0 \ref{digital_recruiting_3} are deployed in the screening process, the more critical it will be to supply machine-readable applicant documents that please the machine learning models used in the screening process.
When this \acs{AI} barrier is successfully surpassed, it must be ensured that the documents also please the decision-making human after reading them.
This will lead to new challenges for the applicants as they must please both the machine-learning models and the human decision-makers.
As the industry-standard formats for applicant documents are the \textit{PDF} and the \textit{Microsoft Word} format, files in such formats must be converted to pure text before they can be supplied to the model.
As this conversion is not the focus of this thesis, it will be briefly discussed in the chapter ref {implementation}.
This also means that a more sophisticated \acs{CV} layout or design and an included portrait photo will not influence the model's output, as the model will only see the text contents.
Future work can incorporate this information by converting this visual information to text using image-to-text models.
The proposed model will compute a score from $0$ to $100$ for each \acs{CV}-job-matchup that will be supplied to it.
The model should also output whether the applicant is promising and should categorize the applicants into two groups.
Based on this score, the \acs{CV}s for a particular job can be ranked from best to worst.
If there is a tie with the output scores, additional criteria will be globally introduced to break the tie.
Furthermore, it is planned that with each score output of the model, an explanation is supplied that clarifies why the model thinks this score is justified.
The exact implementation details of the score and explanation generation are described in chapter \ref{implementation}.
The rankings of the human experts will not be available during the model implementation phase. They will only be used to benchmark the model's accuracy after implementation.

\subsection{Construction Process of the Sample Screening Documents}
At first, there was extensive research on the availability of datasets that contain job descriptions and the corresponding applicant \acs{CV}s in document format. 
With document format, it means that the documents are either supplied in the \textit{PDF} or the \textit{Microsoft Word} format.
Most of the datasets that were found on \textit{Kaggle} \parencite{kaggle}, via the \textit{Google} search or in \textit{GitHub} projects had at least one of the following deficiencies:
\begin{itemize}
    \item The dataset was not publicly available or not anymore publicly available due to legal reasons
    \item The dataset either contained only jobs or only candidates, which is not ideal since random matching is also no option as there will not be completely random applicants for a specific job in the real world
    \item The dataset contained the applicant or job data in a columnar format, which makes it impossible to use it for the human screening process that is used to benchmark the model's accuracy
    \item The dataset only contained a very narrow set of jobs from a particular industry, making it impossible to test the model's ability to generalize to different industries
    \item The dataset contained seemingly random candidates for the contained jobs, which, as noted before, is no good representation of real-world screening data
\end{itemize}
This is why it was decided to construct the sample screening documents for this thesis explicitly.
To have a diverse set of jobs, the following key sectors of the Austrian economy were looked up on the website \parencite{austria_key_sectors}, which an organization of the Austrian Federal Economic Chamber created:
\begin{itemize}
    \item Food and Drink industry
    \item Mechanical and Steel Engineering
    \item Chemical and Automotive industry
    \item Electrics and Electronics industry
    \item Wood, Pulp, and Paper industry
\end{itemize}
By doing some research on the platform \textit{LinkedIn} \parencite{linkedin} on the various job opportunities these industries offer, the following six job types were selected to be contained in the sample screening documents' job descriptions to cover most of Austria's key sectors:
\begin{itemize}
    \item Export Specialist (Food and Drink industry)
    \item Embedded Software Engineer (Electrics and Electronics industry)
    \item Green Hydrogen Piping Engineer (Mechanical and Steel Engineering)
    \item Validation and Qualification Engineer (Chemical and Automotive industry)
    \item Store Manager (picked representatively for the Commerce industry even if it is not a key sector)
    \item Digital Design Engineer (Chemical and Automotive industry)
\end{itemize}
The six job descriptions in the sample screening documents were manually crafted but heavily inspired by job listings from companies on the \textit{LinkedIn} platform to be as close to reality as possible.
All the job descriptions kept a reduced form of the initial job description header structure of the platform, which contains the job title, the job location, the work model, the employment type, and the specific level of the position.
The other content was mainly paraphrased from the real job listings, but the company name was removed from the text, and the job location was always set to \textit{Vienna} to redact the job listings further.
The basic formatting and sectioning of the job listings were kept, but the font and the font sizes of the header and the content were homogenized to make the job listings more standardized.
This is no advantage for the proposed machine learning model as this model only sees the text contents of the job descriptions.

Furthermore, the eight applicants per job description were manually crafted but heavily inspired by real \acs{CV}s from the \textit{LinkedIn} platform.
Possible applicants per job description were searched for on the \textit{LinkedIn} platform by searching for the job title of the job description using a people search.
Then, eight applicants were randomly selected per job description with a proper English \textit{LinkedIn} profile. Another selection criterion was that most of the entries in the education and experience section are commented on and elaborated using English text.
Some candidates listed on later pages of the search results were also selected, which means they had a lower degree of relevance to the job description.
This was done to achieve a reasonable balance of candidate variety and candidate suitability for the job description and to have enough information in the profile to deduce if applicants meet or do not meet specific job requirements.
The \textit{LinkedIn} platform allows the download of profiles as \acs{CV}s in the form of \textit{PDF} files.
The structure, layout, and design of this \acs{CV} download was used to craft the \acs{CV}s for the sample screening documents.
Information that is included in this \acs{CV} structure \label{cv_structure}:
\begin{itemize}
    \item Name
    \item Candidate Location
    \item Summary
    \item Job Experiences
    \item Educational Degrees
    \item Contact Details (not anymore present in the sample screening documents)
    \item Top Skills
    \item Spoken Languages (if present)
    \item Certifications (if present)
    \item Publications (if present)
    \item Honors/Awards (if present)
    \item Patents (if present)
\end{itemize}
To redact the information taken from the \textit{LinkedIn} profiles, all contact-related information was removed from the \acs{CV} documents as well as all hyperlinks present in the \textit{PDF} files were removed.
Moreover, most \acs{CV}s’ candidate locations were changed to \textit{Austria}. Sometimes, this made no sense as some current positions demanded a physical presence elsewhere in the world for the \acs{CV} to appear realistic.
Furthermore, the names of the candidates were redacted to random unisex English names to eliminate the influence of the candidate's sex on the human decision-making process and the model's decision-making process.

\section{Model Design and Model Implementation} \label{implementation}

\subsection{Input Data Preprocessing}
As described in \ref{sample_screening_documents}, all input files are supplied in the \textit{PDF} format, as this format can be used for humans and \acs{AI}-enabled screening and is widely adopted.
The input files can even be printed if a human expert recruiter wants to analyze the person-environment fit while looking at physical paper instead of a digital screen.
This might be more familiar for recruiters if quick notes need to be made.
These \textit{PDF} files must be converted to pure text contents to be tokenizable and supplied to the proposed model.
The text extraction of the \textit{PDF} files was carried out with the \textit{Python} package \textit{PyMuPDF} \parencite{pymupdf}, but faces these issues which are mentioned in \textcite{pymupdf}:
\begin{itemize}
    \item \textbf{Complex File Structure}\\
    The \textit{PDF Reference} defining the \textit{PDF} standard is vast and extensive. 
    The text in a \textit{PDF} document can either be stored in plain text or as a graphic.
    Whereas plain text is easy to extract, the text in a graphic must be extracted using optical character recognition, which may introduce wrongly extracted characters. 
    The \textit{PDF} package supports optical character recognition, but this feature was not used for this thesis.
    Furthermore, for plain text extraction, the font and the used encoding must be adequately detected to extract the text correctly, which is appropriately done by the used package.
    Moreover, plain text segments can be specified as a collection of words and placed on the \textit{PDF} page, which is commonly used. 
    It is also possible to position each character individually on the page.
    Not only that but also the order of the specified plain text segments in the \textit{PDF} file must not occur in the natural reading order of the \textit{PDF} file due to a possible absolute positioning of these segments. The resulting \textit{PDF} file may always be the same even if a random order of these segments is used.
    Due to the final position of the text, a natural reading order may be re-established, but due to the standard's complexity, this is a complex problem. 
    The package offers several extraction modes to cope with this problem.
    \item \textbf{Complex Layouts}\\
    Another facet of the problem described above is the text extraction of \textit{PDF} files with complex layouts.
    If the layout has multiple columns or even a more complex structure, the order of the extracted text segments should be clarified. 
    It usually takes work to determine the natural reading order of this kind of document.
    The used package also offers some extraction modes to cope with this problem.
\end{itemize}
As the \textit{PDF} files containing the job descriptions have a simple layout and no images, no optical character recognition was needed for this case.
These job description \textit{PDF} files were created using \textit{Microsoft Word}, which does not store individual characters as plain text segments but instead stores as long text contents as possible in the natural reading order (from top to bottom) into the resulting \textit{PDF} file.
Most of the time, the plain text segments of a \textit{PDF} file are stored in a natural reading order if the file was programmatically created.
This is also the industry standard way of doing it, so for the extraction, it was enough to extract the plain text segment contents in the order they were defined in the \textit{PDF} file.

The \textit{PDF} files containing the applicant \acs{CV}s were created using \textit{Apache FOP}. They also have no images, so no optical character recognition was needed for this case.
They have a slightly more complex layout than the job description \textit{PDF} files as they feature a two-column layout.
However, the creation tool \textit{Apache FOP} stores the plain text segments of the left column first in the \text{PDF} file, and then the plain text segments of the right column follow in the file.
This means the plain text segments are stored in a natural reading order (from left column to right column, from top to bottom), as is the case for most programmatically generated \textit{PDF} files.
The left \acs{CV} column contains all the information mentioned in \ref{cv_structure} except the name, candidate location, summary, job experiences, and educational degrees.
The extraction was done in the same way as for the job description \textit{PDF} files by extracting the plain text segments in the order they were defined in the \textit{PDF} file.

Optical character recognition would have been needed if there would have been images with text in the \textit{PDF} files or scanned \textit{PDF} documents.
Furthermore, the extraction would have been more complicated if the \textit{PDF} files had had a more complex layout or stored the plain text segments in non-natural reading order.
Text extraction could still have been achieved by using the \textit{PyMuPDF} package and extracting text underneath user-defined extraction rectangles (reading order must be explicitly set), and extracted characters or words can be brought into a natural reading order by ordering them according to the position of their bounding rectangles.

As stated in \textcite{pymupdf}, no effort is made by the package in any way to prettify the extracted text.
That means there can be too few or too many whitespaces (spaces, line breaks, \dots) in expected or unexpected positions in the extracted text.
The non-prettified extracted text was already properly usable for the proposed model, but it was decided that the extracted text should be prettified to group relevant information.
The main issue with the sample screening documents was that the extracted text lacked most of the line breaks that seemed to be there in a \textit{PDF} viewer.
This was the case as line breaks may also be implicitly coded into the \textit{PDF} file by the absolute starting position of the plain text segment.
The prettification was done by sending the extracted text to an \acs{LLM}, which was asked to group relevant text parts in a paragraph and to improve the overall formatting of the text without changing the text contents.
This approach worked reasonably well for the thesis's use case.

\subsection{Model Design}
Following the divide-and-conquer principle of \textcite{pj_fit_ml}, the hard task of programmatically quantizing the person-environment fit for a specific job description and a specific applicant was broken into the following two tasks:
\begin{enumerate}
    \item \textbf{Job Requirement Extraction}\\
    The first part of the proposed model extracted the job requirements into a machine-readable format from a job description that was supplied as pure text.
    All \acs{CV}s from applicants for the same job were matched against the same set of extracted job requirements.
    This adds determinism to the model's output as the same set of job requirements is used for all applicants for the same job.
    \item \textbf{Job Requirement Matching}\\
    The second part of the proposed model matched the extracted job requirements against the applicant's \acs{CV} that was also supplied as pure text.
    This part of the model quantized the person-environment fit between the applicant and the job description for every extracted job requirement.
    Furthermore, the used \acs{LLM} was also asked to state whether the candidate was promising by supplying it with all the results computed in the previous steps.
    Afterward, the quantizations were aggregated into a single score that quantized the person-environment fit between the applicant and the job description in a total score from $0$ to $100$.
\end{enumerate}

\subsection{Job Requirement Extraction} \label{job_requirement_extraction}
This part of the model receives the prettified, extracted text from the job description \textit{PDF} file.
Before extracting the requirements, there were $12$ job requirement types or categories defined, which are heavily inspired by the enumeration of \textcite{job_requirement_types}:
\begin{itemize} \label{job_requirement_types}
    \item \textbf{Work Experience}\\
    This point includes requirements on previous job titles, key responsibilities, achievements, or duration of tenure at a specific position or in a particular field.
    \item \textbf{Education}\\
    For example, this requirement type includes the education level or the requested field of study.
    \item \textbf{Other Qualifications}\\
    This requirement type covers other necessary qualifications like certifications, licenses, or accreditations.
    \item \textbf{Hard Skills}\\
    This requirement type covers the technical skills required to perform the job.
    \item \textbf{Soft Skills}\\
    This requirement type covers the necessary abilities of a candidate to relate well to others.
    \item \textbf{Specific Knowledge}\\
    This requirement type includes knowledge of specific fields necessary to perform the job, which must be listed in the job requirements.
    \item \textbf{Personal Traits}\\
    This requirement type includes personality traits that the employer is looking for to achieve a high person-environment fit and a high person-job fit, which may consist of attention to detail, reliability, creativity, or general intelligence.
    \item \textbf{Languages}\\
    This requirement type includes the languages required to perform well in the job with a necessary level of fluency or proficiency.
    \item \textbf{Travel}\\
    This requirement type includes traveling, apart from commuting, which the job entails.
    \item \textbf{Location}\\
    This requirement type includes the willingness to commute to the working location of the job, but it also consists of the work model like the hybrid or the remote work model.
    \item \textbf{Working Hours}\\
    This requirement type includes the total amount of work hours and the flexibility regarding working hours, including shifts or necessary weekend work.
    \item \textbf{Physical Ability}\\
    This requirement type includes any physical capabilities or limitations relevant to job requirements, such as the ability to lift certain weights or stand for extended periods.
\end{itemize}
The requirements were extracted using one prompt that is sent to the \acs{LLM} that contains the extracted text from the job description as well as a mapping of the job requirement types to a textual definition of the type that should be extracted including some hints how to match the requirements of that type with an applicant \acs{CV}.
This mapping is mostly based on the definitions in \textcite{job_requirement_types} and has the same types as in \ref{job_requirement_types}, but some fine tunings were applied to the definitions to produce a more human-like output.
Moreover, the number of requirement types or their definitions can be easily changed in the current architecture if a human expert wants to modify the model's behavior.
It was decided to extract all the requirements in one pass as extracting them isolated by job requirement type led to many duplicated job requirements.
Also, it seemed like the \acs{LLM} models better categorize all the requirements in one pass.
The prompt requests the \acs{LLM} to output the job requirements extraction result using the \textit{JSON} format with specific fields specified in the prompt.
This \text{JSON} object is then extracted from the output text of the \acs{LLM} and contains $0$ or more requirements as a list per requirement type.
Each requirement has a textual specification and information on whether the requirement is mandatory or optional.
These extracted job requirements by job requirement types are then passed onto the job requirement matching task.

\subsection{Job Requirement Matching} \label{job_requirement_matching}
The job requirements extracted in the previous step are then matched against each applicant's \acs{CV} that is also already supplied as pure text because it already had gone through the preprocessing.
The $0$ or more requirements per requirement type are matched individually against each applicant's \acs{CV}, so the matching processes do not interfere with each other and can be parallelized.
The matching is done by querying the \acs{LLM} to output a \textit{JSON} object that contains a score between $0$ and $100$ and an explanation for the score by supplying the \acs{LLM} with the job requirement specification, the job requirement type, the job requirement type definition from \ref{job_requirement_extraction} and the applicant's \acs{CV}.
The score was chosen between the interval of $0$ and $100$ as the model could better output intermediate scores in this interval more than in the interval of $0$ and $1$, for example.
This is likely because most sample text also contained percentage values instead of values between $0$ and $1$.
These objects are then extracted from the text output of the \acs{LLM} and are used to amend the data structure created in the previous step with the concrete score and a textual explanation.
Afterward, this amended data structure with scores and explanations was used as input to ask the \acs{LLM} if the applicant is promising or not.
The model was prompted to output the result again as \textit{JSON} object, which contained whether the applicant is promising and a textual explanation for the decision.
Furthermore, the applicant's total score was computed by first taking an arithmetic mean of each requirement's score within each requirement type. Job requirement types that had $0$ requirements were left out.
Secondly, a weighted arithmetic mean was taken from the arithmetic means of each requirement type to build the final total score of the applicant, which is also between $0$ and $100$.
A correction factor was used to compensate for requirement types with $0$ requirements and a non-zero weighting. The factor was necessary to still cover the total score range from $0$ to $100$ even if one requirement type had no requirements.
This final total score can be used to sort the applicants by relevance, which means predicted person-environment fit.
The weighting used for the total score computation was initially set to a fixed value. Still, the weighting of the individual requirement types can be easily changed if a human expert wants to change the model's behavior.
This concludes the inner workings of the requirement matching task, and the final output of the model per applicant is the amended data structure from before, a total score, a promising or not promising decision, and a textual explanation for the decision.

\subsection{Output Data Postprocessing}
In this step, the model's output data structure is presented to the user in a human-readable way.
This was done by providing the user a table per job description that includes all candidates, their total scores, whether the candidate is promising, and the textual explanation for this decision.
Furthermore, this table is sorted in descending order by the promising flag first and then by the total score of the applicants.
This table is provided in the console output of the invoked model, and the table is also offered as a \textit{CSV} file to be used as input for other software in the human resource pipeline.
It must be noted that there can be a promising candidate that has a lower total score than a non-promising candidate, as the decision whether a candidate is promising or not is not solely based on the total score but also the individual scores of the requirements and whether they are mandatory or not.
That means focusing the work effort on the most promising candidates, and it is advisable to look at the candidates from top to bottom in the table due to the sorting above.
Moreover, suppose the recruiter wants to examine how the decision was made. In that case, the model stores each applicant's whole internal data structure in the human-readable and machine-readable \textit{JSON} format.
Compared to the table, this \textit{JSON} file also contains information on the matched individual requirements, including their score and textual explanations.
That should help to fine-tune the job requirement type definitions and the weightings of the job requirements to match the desires of the human expert recruiter. 

\subsection{Supported \acs{LLM}s}
Whenever the section \ref{implementation} mentions the use of an \acs{LLM}, it means that a concrete \acs{LLM} implementation is used.
For this thesis, only \acs{LLM}s that were fine-tuned for chat completions were used, as the prompts are formulated in conversational requests.
An Interface-based implementation supports different models from different vendors, regardless of whether they are open-source or proprietary.
Requests to third-party hosting providers are used to query all models. The interface-based implementation's configuration options are automatically translated to options understandable by the underlying and currently used model.
The following models were implemented and can be used as \acs{LLM} for the proposed model:
\begin{enumerate}
    \item \textbf{GPT-4 Turbo}\\
    This proprietary model is an evolution of \textit{GPT-4} proposed in \textcite{gpt4}, which has not gotten its technical report. It has a context window of $128000$ tokens and an undisclosed amount of parameters.
    The model was developed by \textit{OpenAI} and is hosted by \textit{OpenAI}.
    \item \textbf{GPT-3.5 Turbo}\\
    This proprietary model is an evolution of \textit{GPT-3} proposed in \textcite{gpt3}, which has not gotten its technical report. It has a context window of $16385$ tokens and an undisclosed amount of parameters.
    The model was developed by \textit{OpenAI} and is hosted by \textit{OpenAI}.
    \item \textbf{LLAMA 2-Chat}\\
    This open-source model collection is described in \textcite{llama2}. Each model has a context window of $4096$ tokens, and the three models of this collection used in this thesis differ in their number of parameters, which are either $7$, $13$, or $70$ billion parameters.
    These models were developed by \textit{Meta} and are hosted by \textit{Replicate} in the implementation's case, one of many hosting providers for this model collection.
    Due to the open-source nature of this model collection, it can be operated self-hosted without sending data to third parties.
    \item \textbf{Gemini}\\
    This proprietary model collection is described in \textcite{gemini}. Currently, only \textit{Gemini Pro} is available publicly. \textit{Gemini Pro} has a context window of $32000$ tokens and an undisclosed amount of parameters.
    These models were developed by \textit{Google} and are hosted by \textit{Google}.
\end{enumerate}

\subsection{Context Windows of \acs{LLM}s}
The context window must be large enough to contain the prompt itself, including all input texts and the expected output text of the model, to reference all input parts in the answer fully.
As a rule of thumb, around four characters of standard English text will be translated to one token, a text fragment that is encoded using a unique integer, in the case of the \textit{OpenAI} tokenizer, which is based on the byte pair encoding algorithm \parencite{openai_tokenizer}. 
This token translation is bidirectional and can, therefore, go both ways, which means the unique integer sequence can be translated back into the source text.
As mentioned in \textcite[6]{llama2}, the \textit{LLAMA 2-Chat} models also use a tokenizer that employs the byte pair encoding algorithm. Therefore, the rule of thumb for the \textit{OpenAI} tokenizer is used as a general measure in the following calculation examples.
The largest \acs{CV} that this thesis uses as input text has around $6500$ characters, translating to about $1625$ tokens.
The most extensive job description this thesis uses as input text consists of around $3700$ characters, translating to about $925$ tokens.
The largest prompt without the contained input text is the one that extracts the requirements from the job description as it contains all the job requirement definitions and consists of around $7000$ characters in the current default configuration, which equals about $1750$ tokens.
That means with a context size of $4096$ tokens, only $4096-925-1750=1421$ tokens are left for the output text of the model for the job requirement extraction task, which equals about $5684$ characters.
The longest answer that the used \acs{LLM} may give depends on when the model is stopping to generate output text, which itself depends on how many requirements are present in the input job description.
A typical requirement extraction \acs{LLM} output contains around $3000$ characters, so it is within the remaining and available minimum context size of all supported models, and there is still some margin there.
Regarding the context window size, the job requirement extraction task is the limiting factor or critical path, as the other prompts without input texts and expected output texts are much smaller.
If the configuration is changed such that the job requirement definitions are more extensive or if the used job descriptions are much larger, the context window size may quickly reach its limit. Models with a larger context size must be used in this case.

\subsection{Determinism of \acs{LLM}s}
The settings used for configuring the \acs{LLM}s were picked to make the underlying \acs{LLM} deterministic, which means that the same input to the model should lead to the same output every single time.
These settings remove all configurable penalties (presence penalty, repetition penalty, repetition penalty) and biases (only logit bias) that would alter the model output, use a constant system prompt, limit the maximum output token amount to $4096$, and define no stop sequences.
The settings above are not altering if the \acs{LLM} is behaving deterministically or not, but they are instead listed for completeness.
The output tokens of an \acs{LLM} are sampled from the probability distribution of the model's output logits, representing the token vocabulary.
This sampling needs a random number generator to sample, which is seeded with a known fixed seed to make the output values of this generator and, therefore, the sampled tokens deterministic.
This single setting makes an \acs{LLM} deterministic.
Additionally, the token vocabulary sampling space sampled in every \acs{LLM} step can be trimmed down by setting the number of top tokens to sample from or by only sampling from the top tokens that add up to a certain probability mass.
Both settings were configured such that only the single top token is sampled in every step, which means the token with the highest probability should always be chosen.
The temperature setting of \acs{LLM}s is the input to a \textit{Softmax} function with temperature. This parameter controls whether the probability differences should be softened or enlarged.
In this thesis, the temperature was picked to zero, which means only the highest probability token has a non-zero probability and, therefore, should always be chosen.
Despite fixing the seed, which should make the model deterministic anyway, trimming the sampling space to only one token and setting the temperature setting to enforce this single token further, only the \textit{LLAMA 2-Chat} models were behaving deterministically.
Both models from \textit{OpenAI} failed to deliver deterministic responses even with the settings above, which should lead to deterministic behavior.
The responses from \textit{OpenAI} models to the same inputs are primarily similar or identical. Still, nothing can be guaranteed. Therefore, no unit test cases can be written for non-deterministic models if they check the full model response text.
The \textit{OpenAI} API sends a \textit{system fingerprint} value with their responses and does not guarantee determinism in general \parencite{openai_chat_api}.
\textit{OpenAI} systems try to sample deterministically if the seed was also fixed in the request \parencite{openai_chat_api}.
If the \textit{system fingerprint} of the response changes compared to a previous request with the same input, it will likely result in a non-deterministic response \parencite{openai_chat_api}.
The problem for the \textit{OpenAI} API user is that there is no way to specify which system you want to have to get deterministic responses. \textit{OpenAI} assigns the used backend systems nondeterministically. Two back-to-back requests will likely have two different \textit{system fingerprint} values \parencite{openai_chat_api}.
The nondeterministic behavior is known to \textit{OpenAI}, and hopefully, they will fix it in a future release.
The \textit{Gemini} model family suffered from similar issues as the \textit{OpenAI} models. It does not even support seeding the internally used random number generator.

\subsection{Model Configuration} \label{model_configuration}
This is the default configuration that is used to operate the proposed model. Every single setting can be changed to modify the behavior of the proposed model to the desires of the operating human expert recruiter:
\lstinputlisting[language=TypeScript,label=lst:default_model_configuration,caption=Default configuration of the proposed model]{../hrgpt/default_config.json}

\subsection{Scope and Limitations}
As \acs{LLM}s are trained on massive amounts of data compressed into the model's parameters, the resulting model can summarize text and answer questions based on the input text reasonably well in the most general cases.
As most text data on the internet is written in English, the proposed model in its first version was only tested and used with English job descriptions and \acs{CV}s as English is the preferred language of most if not all \acs{LLM}s.
Due to this compression, the model is only guaranteed to output plausible text in some cases. 
As there are vast amounts of possible input texts, the model cannot summarize and answer questions on all of them correctly or with the same accuracy, as \acs{LLM}s do not understand the text the same way humans do.
This means that the model will not be able to give a plausible output on every theoretically possible \acs{CV} job matchup and should, therefore, not be used as a single point of truth to pre-filter applicants in the screening process if fairness is a top priority.
However, they will behave quite robustly in most cases due to the vast amounts of training data that are scraped off the internet and used to train these models.
Furthermore, the introduced model will not be fair as the human training data used to build \acs{LLM}s already has some bias within its data. The decision-making process of humans is ambiguous, and past experiences influence humans’ decisions. This makes it very hard even for the legal systems to detect discrimination \parencite[113]{discrimination_algorithms}. That means that human decision-making is not fair in all cases. But \textcite[113]{discrimination_algorithms} sees a positive force for equity by incorporating algorithms for decision-making and detecting discrimination if fairness can be reasonably defined. Proving an algorithm's fairness in all possible cases if its behavior depends on human-written data, like in the thesis case, is virtually impossible. Also, in \textcite[158-160]{discrimination_algorithms}, the authors pointed out that race-aware predictors are more capable of determining college success than race-blind predictors.
That means when the law forbids the usage of certain features in algorithmic or human decision-making in terms of fairness, the loss of accuracy can be significant, as shown in the example. Other application areas like healthcare might be where race-aware algorithms could save lives, but more restrictive legislation would prevent their usage.

\subsection{Multilingual Support}
Multilingual support for the proposed model was introduced by translating all extracted non-English text from job descriptions or \acs{CV}s into English using a machine translation service before supplying it to the underlying \acs{LLM}.
This has the advantage that the \acs{LLM}s can still operate in their preferred language domain.
The detection of whether a text is non-English and needs translation was done by using the \textit{Python} package \textit{Lingua Language Detector} \parencite{lingua_language_detector}, as it is an open-source language detector that offers similar performance to the language detection accuracy of \textit{Google Translate} for example.
For example, the used machine translation service could be either \textit{DeepL Translate} \parencite{deepl_translate} or \textit{Google Translate} \parencite{google_translate}, which both have an API to better integrate with computer programs.
The model implementation only supports translations with \textit{Google Translate}.
If sending data to third parties is not desired, an open-source and offline machine translation library like \textit{Argos Translate} \parencite{argos_translate} could be used and integrated.
That means that all critical building blocks of the model can be replaced with open-source and offline alternatives if operating the model in a more privacy-preserving way is desired.
Furthermore, the English text contents of the \textit{CSV} and \textit{JSON} output files can be optionally translated to a configurable non-English output language using a machine translation service.
This output language can be changed by altering the configuration of the proposed model \ref{model_configuration}.

\subsection{Multiple File Type Support}
The thesis only focused on input documents in the \textit{PDF} format.
However, the tool was also programmed to support \textit{Word} and plain text files as input documents.

\chapter{Data}

\section{Data Collection}

\subsection{Qualitative Research}

\subsubsection{Kick-Off Meeting Discussion Guide}
To provide more validity to the open, verbal discussion with the human experts of the focus group, the following discussion guide formulates the main topics and the most important open questions in the form of a questionnaire as quickly outlined in \ref{kick_off_meeting}:
\begin{enumerate}
    \item \textbf{Bias and Fairness}
    \begin{itemize}
        \item How do you perceive the potential for bias or definitive bias in \acs{AI}-assisted recruitment technologies?
        \item How important is it that the used \acs{AI} system is designed to ensure fairness and avoid discrimination based on gender, ethnicity, age, or other non-job-related factors?
        \item What are your views on the ethical considerations surrounding using machine learning in human resource processes?
    \end{itemize}
    \item \textbf{Accuracy and Reliability}
    \begin{itemize}
        \item To what extent should these technologies replicate human judgment in the screening process?
        \item To what extent have different \acs{CV} formats or layouts influenced the screening decision? Is an \acs{AI} model that uses the text contents of the \acs{CV} sufficient or not? What important information could be omitted with this approach?
        \item What difficulties do you see for \acs{AI} systems that can effectively match job requirements with a diverse range of qualifications and experiences across different industries?
    \end{itemize}
    \item \textbf{Legal Considerations}
    \begin{itemize}
        \item What legal challenges do you foresee in implementing \acs{AI} in \acs{HR} processes, particularly concerning data security and privacy?
        \item How can we maintain transparency in \acs{AI}-assisted recruitment technologies? 
        \item Would the perceived transparency be better if the \acs{AI} software runs on the user's servers? Moreover, would you send applicant documents to third-party servers, or would you prefer to run the \acs{LLM} software on your servers?
        \item What key legal safeguards should be in place when using these technologies?
    \end{itemize}
    \item \textbf{Machine Learning as Assistive Technology}
    \begin{itemize}
        \item How do you see machine learning complementing human decision-making in \acs{HR}?
        \item Are there scenarios where \acs{AI}-assisted technologies should not be used in recruitment?
        \item What balance should be struck between automated and human decision-making in the screening process?
        \item What are your expectations regarding the efficiency gains from these technologies?
    \end{itemize}
    \item \textbf{Customization and Adaptability}
    \begin{itemize}
        \item How important is customization in \acs{AI}-assisted recruitment technologies for different industries and company cultures?
        \item What aspects of the technology should be customizable by the user?
        \item Should the user be able to tweak the model's configuration of requirement extraction, requirement matching, and weighting of the requirement match scores to change the model output, or is a more high-level form of customizability desired?
    \end{itemize}
    \item \textbf{User Experience}
    \begin{itemize}
        \item What are the key factors that make an \acs{AI}-assisted recruitment tool user-friendly?
        \item How should these technologies integrate with existing \acs{HR} systems, and do these systems support an export/import functionality to/from third-party systems? Moreover, what are the dominant file types used for applicant documents?
        \item What are your thoughts on third-party tools in HR? Do you prefer a fully integrated solution, or are multiple tools already used?
    \end{itemize}
    \item \textbf{Candidate Experience}
    \begin{itemize}
        \item What advantages do \acs{AI}-assisted recruitment technologies offer to candidates?
        \item How might these technologies improve response times and feedback generation for applicants?
        \item What impact do you think \acs{AI} screening will have on the overall candidate experience?
    \end{itemize}
    \item \textbf{Previous Experiences with \acs{AI}-assisted Technologies in Human Resources}
    \begin{itemize}
        \item What has been your experience with \acs{AI}-assisted technologies in \acs{HR} to date?
        \item Can you share insights on the general reception, advantages, disadvantages, and areas of improvement for these technologies?
        \item How have these technologies been integrated into the \acs{HR} pipeline in your experience, and in what part of the \acs{HR} process?
    \end{itemize}
    \item \textbf{Costs}
    \begin{itemize}
        \item How do you assess the affordability and cost-effectiveness of \acs{AI}-assisted recruitment technologies?
        \item What are your expectations regarding the investment required for the model outlined in this thesis?
        \item How do you justify the costs of these technologies with their benefits?
    \end{itemize}
\end{enumerate}

\subsubsection{Closure Meeting Discussion Guide}
To provide more validity to the open, verbal discussion with the human experts of the focus group, the following discussion guide formulates the main topics and the most important open questions in the form of a questionnaire as quickly outlined in \ref{closure_meeting}:
\begin{enumerate}
    \item \textbf{Consistency and Agreement}
    \begin{itemize}
        \item How do you perceive the consistency between the model's categorization and ordering of applicants compared to the human recruiters' approach? Can you provide specific examples?
        \item Are you satisfied with the accuracy achieved by the model in both the categorization and the ordering tasks? Why or why not?
        \item Did you notice any biases in the model's output? If so, how do these biases compare to human recruiters' biases?
        \item Were there any outliers in the model's results? What do you think could be the reasons behind these outliers?
    \end{itemize}
    \item \textbf{Explainability and Transparency}
    \begin{itemize}
        \item How would you rate the model's ability to explain its decision-making process transparently and understandably?
        \item Can you give an example of a situation where the model's explanation of its decision was particularly clear or unclear?
        \item Upon reviewing the model's explanations, did you identify any deficiencies? If so, what were they?
        \item In your opinion, what changes or tweaks could be made to the model's configuration to improve understanding and usability?
        \item How important do you believe explainability and transparency are in using \acs{AI} for recruitment? Do you think these factors impact user trust and confidence?
    \end{itemize}
\end{enumerate}

\subsection{Quantitative Research}

\section{Data Overview}
\lipsum[1]

\chapter{Analysis}

\section{Accuracy of the Model}

\subsection{Categorization Accuracy}
\lipsum[1]

\subsection{Ranking Accuracy}
\lipsum[1]

\section{Explainability of the Model Results}
\lipsum[1]

\section{Experiences with \acs{AI}-Assisted Technologies in Human Resources}
\lipsum[1]

\section{Considerations on \acs{AI}-Assisted Technologies in Human Resources}
\lipsum[1]

\section{Expectations from \acs{AI}-Assisted Technologies in Human Resources}
\lipsum[1]

\section{Visualizations}
\lipsum[1]

\chapter{Discussion and Conclusion}

\section{Interpretation}
\lipsum[1]

\section{Machine Learning Benefits}
\lipsum[1]

\section{Challenges and Ethical Considerations}
\lipsum[1]

\section{Future Implications}
\lipsum[1]

\section{Summary}
\lipsum[1]

\section{Contributions}
\lipsum[1]

\section{Practical Implications}
\lipsum[1]

\backmatter

% Use an optional list of algorithms.
% \listofalgorithms
% \addcontentsline{toc}{chapter}{List of Algorithms}

% Add a bibliography.
% comment out when finished
\nocite{*}
\printbibliography

% Add an index.
\printindex

% Add a glossary.
\printglossaries


\chapter{Appendix}
\setcounter{page}{1}
\setcounter{chapter}{0}

\section{Code Listings}

\lstinputlisting[language=Python,label=lst:code_usage_example,caption=Example usage of the programmed model]{../hrgpt/utils/chat_utils.py}

\end{document}
